# -*- coding: utf-8 -*-
"""RNN_seq_seq.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kid5J-3n8bSTKGnwzU2BmYwCi3GMYD_5
"""

# !pip install arabert

# from google.colab import drive
# drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive

# -*- coding: utf-8 -*-
"""
Created on Thu Aug 10 10:25:24 2023

@author: ayaha
"""
# from __future__ import unicode_literals, print_function, division
# from arabert.preprocess import ArabertPreprocessor
# from Data_preprocessing import data_preprocessing


from torch.optim.lr_scheduler import ReduceLROnPlateau
import math
import time
from sklearn.model_selection import train_test_split
import pandas as pd
from torch.utils.data import TensorDataset, DataLoader, RandomSampler
import numpy as np
from io import open
import unicodedata
import re
import random
import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F
from transformers import (AutoModel, AutoTokenizer)
MAX_LENGTH = 200
model_name = "aubmindlab/bert-base-arabertv02-twitter"
# arabic_prep = ArabertPreprocessor(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
SOS_token = tokenizer.cls_token_id
EOS_token = tokenizer.sep_token_id
PAD_token = tokenizer.pad_token_id
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)
data_Gulf = pd.read_csv("./data_Gulf_preprocessed_train", sep="\t")
data_Iraqi = pd.read_csv("./data_Iraqi_preprocessed_train", sep="\t")
data_Levantine = pd.read_csv("./data_Levantine_preprocessed_train", sep="\t")
data_Nile_Basin = pd.read_csv("./data_Nile_Basin_preprocessed_train", sep="\t")
data_North_Africa = pd.read_csv(
    "./data_North_Africa_preprocessed_train", sep="\t")

data_Gulf_valid = pd.read_csv("./data_Gulf_preprocessed_valid", sep="\t")
data_Iraqi_valid = pd.read_csv("./data_Iraqi_preprocessed_valid", sep="\t")
data_Levantine_valid = pd.read_csv(
    "./data_Levantine_preprocessed_valid", sep="\t")
data_Nile_Basin_valid = pd.read_csv(
    "./data_Nile_Basin_preprocessed_valid", sep="\t")
data_North_Africa_valid = pd.read_csv(
    "./data_North_Africa_preprocessed_valid", sep="\t")

data_Gulf_test = pd.read_csv("./data_Gulf_preprocessed_test", sep="\t")
data_Iraqi_test = pd.read_csv("./data_Iraqi_preprocessed_test", sep="\t")
data_Levantine_test = pd.read_csv(
    "./data_Levantine_preprocessed_test", sep="\t")
data_Nile_Basin_test = pd.read_csv(
    "./data_Nile_Basin_preprocessed_test", sep="\t")
data_North_Africa_test = pd.read_csv(
    "./data_North_Africa_preprocessed_test", sep="\t")

domain = ["Gulf", "Iraqi", "Levantine", "Nile_Basin", "North_Africa"]
data_train = []
for d in domain:
    data = pd.read_csv("./data_{}_preprocessed_train".format(d), sep="\t")
    data_train.append(data)

data_1 = pd.concat([data_train[0], data_train[1]], ignore_index=True)
data_2 = pd.concat([data_train[2], data_train[3]], ignore_index=True)

data_train_src = pd.concat([data_1, data_2], ignore_index=True)
data_train_all = pd.concat([data_train_src, data_train[4]], ignore_index=True)
data_train_trgt = data_train[4]

# domain = ["Gulf", "Iraqi", "Levantine", "Nile_Basin", "North_Africa"]
data_test = []
for d in domain:
    data = pd.read_csv("./data_{}_preprocessed_test".format(d), sep="\t")
    data_test.append(data)

data_1 = pd.concat([data_test[0], data_test[1]], ignore_index=True)
data_2 = pd.concat([data_test[2], data_test[3]], ignore_index=True)

data_test_all = pd.concat([data_1, data_2], ignore_index=True)
data_test_all = pd.concat([data_test_all, data_test[4]], ignore_index=True)

# domain = ["Gulf", "Iraqi", "Levantine", "Nile_Basin", "North_Africa"]
data_valid = []
for d in domain:
    data = pd.read_csv("./data_{}_preprocessed_valid".format(d), sep="\t")
    data_valid.append(data)

data_1 = pd.concat([data_valid[0], data_valid[1]], ignore_index=True)
data_2 = pd.concat([data_valid[2], data_valid[3]], ignore_index=True)

data_valid_src = pd.concat([data_1, data_2], ignore_index=True)
data_valid_all = pd.concat([data_valid_src, data_valid[4]], ignore_index=True)

# print(data_train_src.shape, data_train_all.shape)

data_1 = pd.concat([data_valid_all, data_train_all], ignore_index=True)
data_all = pd.concat([data_test_all, data_1], ignore_index=True)



data_train_all.to_csv("./data_train_all", sep="\t")
data_test_all.to_csv("./data_test_all", sep="\t")
data_valid_all.to_csv("./data_valid_all", sep="\t")
data_all.to_csv("./data_all", sep="\t")

data_file = "./data_train_all"
vocab_file = "./data_all"
# data_trgt_file = "./Data/data_trgt_train_MAG"
data_valid_file = "./data_valid_all"


class Lang:
    def __init__(self, name):
        self.name = name
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.word2index = {
            self.tokenizer.pad_token: self.tokenizer.pad_token_id}
        self.word2count = {}
        self.index2word = {
            self.tokenizer.pad_token_id: self.tokenizer.pad_token}
        self.n_words = 1  # Count PAD token

    def addSentence(self, sentence):
        for word in self.tokenizer.tokenize(sentence, add_special_tokens=True):
            self.addWord(word)

    def addWord(self, word):
        if word not in self.word2index:
            self.word2index[word] = self.tokenizer.convert_tokens_to_ids(word)
            self.word2count[word] = 1
            self.index2word[self.tokenizer.convert_tokens_to_ids(word)] = word
            self.n_words += 1
        else:
            self.word2count[word] += 1

# Turn a Unicode string to plain ASCII, thanks to
# https://stackoverflow.com/a/518232/2809427


def unicodeToAscii(s):
    return ''.join(
        c for c in unicodedata.normalize('NFKC', s)
        if unicodedata.category(c) != 'Mn'
    )

# Lowercase, trim, and remove non-letter characters


def normalizeString(s):
    s = unicodeToAscii(s.lower().strip())
    # s= arabic_prep.preprocess(s)
    s = re.sub(r"([?.!,¿])", r" \1 ", s)
    s = re.sub(r'[" "]+', " ", s)
    s = re.sub(r"[^a-zA-Z؀-ۿ?.!,¿]+", " ", s)
    s = re.sub(r"([.!?])", r" \1", s)
    return s

def readLangs(lang1, lang2, reverse=False, label="src"):

    print("Reading lines...")

    # Read the file and split into lines
    if label == "vocab":
        # lines = open(vocab_file, encoding='utf-8').\
        # read().strip().split('\n')
        data = data_all
    if label == "src":
        # lines = open(data_file, encoding='utf-8').\
        #     read().strip().split('\n')
        data = data_train_src
    # if label =="trgt":
    #     lines = open(data_trgt_file, encoding='utf-8').\
    #     read().strip().split('\n')
    if label == "valid":
        # lines = open(data_valid_file, encoding='utf-8').\
        # read().strip().split('\n')
        data = data_valid[4]
    if label =="trgt":
        data = data_train_trgt

    # Split every line into pairs and normalize
    # pairs = [[normalizeString(s) for s in l.split('\t')] for l in lines]
    pairs = [[normalizeString(data.source_lang[idx]),
              normalizeString(data.target_lang[idx])]
             for idx in data.index]
    # Reverse pairs, make Lang instances
    if reverse:
        pairs = [list(reversed(p)) for p in pairs]
        input_lang = Lang(lang2)
        output_lang = Lang(lang1)
    else:
        input_lang = Lang(lang1)
        output_lang = Lang(lang2)

    return input_lang, output_lang, pairs


def prepareData(lang1, lang2, reverse=False, label="src"):
    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse, label)
    print("Read %s sentence pairs" % len(pairs))
    pairs = pairs[1:]
    print("Trimmed to %s sentence pairs" % len(pairs))
    print("Counting words...")
    for pair in pairs:
        input_lang.addSentence(pair[0])
        input_lang.addSentence(pair[1])
    output_lang = input_lang
    print("Counted words:")
    print(input_lang.name, input_lang.n_words)
    print(output_lang.name, output_lang.n_words)
    return input_lang, output_lang, pairs


def indexesFromSentence(lang, sentence):
    return [lang.word2index.get(word, 0) for word in tokenizer.tokenize(sentence)]


def tensorFromSentence(lang, sentence):
    indexes = indexesFromSentence(lang, sentence)
    indexes.append(EOS_token)
    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)


def get_dataloader(batch_size, label="src"):
    input_lang, output_lang, _ = prepareData('ar', 'arz', label="vocab")

    _, _, pairs = prepareData('ar', 'arz', label=label)
    # elif label == "trgt":
    #     _, _, pairs = prepareData('ar', 'arz', label="trgt")

    pairs = pairs[1:]
    n = len(pairs)
    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)
    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)

    for idx, (inp, tgt) in enumerate(pairs):
        inp_ids = indexesFromSentence(input_lang, inp)
        tgt_ids = indexesFromSentence(output_lang, tgt)
        inp_ids.append(EOS_token)
        tgt_ids.append(EOS_token)
        input_ids[idx, :len(inp_ids)] = inp_ids
        target_ids[idx, :len(tgt_ids)] = tgt_ids

    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),
                               torch.LongTensor(target_ids).to(device))

    train_sampler = RandomSampler(train_data)
    train_dataloader = DataLoader(
        train_data, sampler=train_sampler, batch_size=batch_size)
    return input_lang, output_lang, train_dataloader


def beam_search(encoder, decoder, sentence, input_lang, output_lang, beam_width=5, max_length=MAX_LENGTH):
    with torch.no_grad():
        sentence = normalizeString(sentence)
        input_tensor = tensorFromSentence(input_lang, sentence)

        encoder_outputs, encoder_hidden = encoder(input_tensor)
        decoder_outputs, decoder_hidden, decoder_attn = decoder(
            encoder_outputs, encoder_hidden)

        # Initialize the list to hold the completed beams
        completed_beams = []

        # Start the beam search process

        candidate_beams = []
        decoder_hidden = encoder_hidden

        # Get the top-k candidates and their probabilities
        topk_probs, topk_indices = decoder_outputs.squeeze().topk(beam_width)
        topk_probs = topk_probs.T  # (beam, max_length)
        topk_indices = topk_indices.T

        # Expand the current beam with the top-k candidates

        for i in range(beam_width):
            decoder_output = []
            candidate_prob = 0.0
            for j in range(MAX_LENGTH):

                new_candidate_idx = topk_indices[i][j].item()
                decoder_output.append(new_candidate_idx)

                candidate_prob += topk_probs[i][j].item()
                new_decoder_input = torch.tensor(
                    [decoder_output], device=device)
                new_beam_score = (candidate_prob)

                # Check if the candidate is the end token
                if new_candidate_idx == EOS_token:
                    completed_beams.append((decoder_output, new_beam_score))
                    break
                if j == (MAX_LENGTH-1):
                    completed_beams.append((decoder_output, new_beam_score))

        # Sort the completed beams and select the one with the highest score
        completed_beams.sort(key=lambda x: x[1], reverse=True)

        decoded_words = [output_lang.index2word.get(
            word, tokenizer.unk_token)for word in completed_beams[0][0]]

        return decoded_words  # No attention scores for beam search


def evaluateRandomly(encoder, decoder, input_lang, output_lang, data=None, beam=1, n=None):
    outputs = []
    targets = []
    i = 0
    for idx in data.index:
        if n != None:
            if i == n:
                break

        targets.append([normalizeString(data.target_lang[idx])])
        # print('input', data.source_lang[idx])
        # print('output', data.target_lang[idx])
        output_words = beam_search(
            encoder, decoder, data.source_lang[idx], input_lang, output_lang, beam_width=beam)

        output_sentence = tokenizer.convert_tokens_to_string(output_words)
        output_sentence = output_sentence.replace("[SEP]", "")
        output_sentence = output_sentence.replace("[PAD]", "")
        output_sentence = output_sentence.replace("[CLS]", "")
        output_sentence = output_sentence.replace("[UNK]", "")
        outputs.append(output_sentence)

        # print('prediction', output_sentence)
        # print('')
        i = i+1
    return targets, outputs


def load_checkpoit(checkpoint_file, model, optimizer):
    print("Loading checkpoint...")
    checkpoint = torch.load(checkpoint_file)
    model.load_state_dict(checkpoint["model_state_dict"])
    optimizer.load_state_dict(checkpoint["optimizer_state_dict"])


def save_checkpoit(checkpoint_file, model, optimizer):
    print("Saving checkpoint...")
    torch.save({

        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),

    }, checkpoint_file)


def asMinutes(s):
    m = math.floor(s / 60)
    s -= m * 60
    return '%dm %ds' % (m, s)


def timeSince(since, percent):
    now = time.time()
    s = now - since
    es = s / (percent)
    rs = es - s
    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))

# encoder class


class EncoderRNN(nn.Module):
    def __init__(self, input_size, hidden_size, dropout_p=0.1):
        super(EncoderRNN, self).__init__()
        self.hidden_size = hidden_size

        self.embedding = nn.Embedding(input_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)
        self.dropout = nn.Dropout(dropout_p)

    def forward(self, input):
        embedded = self.dropout(self.embedding(input))

        output, hidden = self.gru(embedded)
        return output, hidden

# attention classes


class BahdanauAttention(nn.Module):
    def __init__(self, hidden_size):
        super(BahdanauAttention, self).__init__()
        self.Wa = nn.Linear(hidden_size, hidden_size)
        self.Ua = nn.Linear(hidden_size, hidden_size)
        self.Va = nn.Linear(hidden_size, 1)

    def forward(self, query, keys):
        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))
        # scores shape (N,L,1)
        scores = scores.squeeze(2).unsqueeze(1)

        weights = F.softmax(scores, dim=-1)
        context = torch.bmm(weights, keys)
        # weights shape is N,1,L
        # context shape is N,1,hid_size
        return context, weights


class DotAttention(nn.Module):
    def __init__(self):
        super(DotAttention, self).__init__()

    def forward(self, query, keys):
        scores = torch.bmm(query, keys.permute(0, 2, 1))

        weights = F.softmax(scores, dim=-1)
        context = torch.bmm(weights, keys)
        # weights shape is N,1,L
        # context shape is N,1,hid_size
        return context, weights


teacher_force_ratio = 0.5


class AttnDecoderRNN(nn.Module):
    def __init__(self, hidden_size, output_size, dropout_p=0.1, attention="Bahdanau"):
        super(AttnDecoderRNN, self).__init__()
        self.embedding = nn.Embedding(output_size, hidden_size)

        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)
        self.out = nn.Linear(hidden_size, output_size)
        self.dropout = nn.Dropout(dropout_p)
        if attention == "Bahdanau":
            self.attention = BahdanauAttention(hidden_size)
        else:
            self.attention = DotAttention()

    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None, MAX_LENGTH=MAX_LENGTH):
        batch_size = encoder_outputs.size(0)
        decoder_input = torch.empty(
            batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)
        decoder_hidden = encoder_hidden
        decoder_outputs = []
        attentions = []

        for i in range(MAX_LENGTH):
            decoder_output, decoder_hidden, attn_weights = self.forward_step(
                decoder_input, decoder_hidden, encoder_outputs
            )
            decoder_outputs.append(decoder_output)
            attentions.append(attn_weights)
            use_teacher_forcing = True if random.random() < teacher_force_ratio else False
            if target_tensor is not None:
                # Teacher forcing: Feed the target as the next input
                if use_teacher_forcing:
                    decoder_input = target_tensor[:, i].unsqueeze(
                        1)  # Teacher forcing
                else:
                    _, topi = decoder_output.topk(1)
                    decoder_input = topi.squeeze(-1).detach()
            else:
                # Without teacher forcing: use its own predictions as the next input
                _, topi = decoder_output.topk(1)
                decoder_input = topi.squeeze(-1).detach()

        decoder_outputs = torch.cat(decoder_outputs, dim=1)
        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)
        attentions = torch.cat(attentions, dim=1)

        return decoder_outputs, decoder_hidden, attentions

    def forward_step(self, input, hidden, encoder_outputs):
        embedded = self.dropout(self.embedding(input))

        query = hidden.permute(1, 0, 2)
        context, attn_weights = self.attention(query, encoder_outputs)
        input_gru = torch.cat((embedded, context), dim=2)

        output, hidden = self.gru(input_gru, hidden)
        output = self.out(output)

        return output, hidden, attn_weights

from torch.autograd import Function
class GradientReversalFn(Function):
    @staticmethod
    def forward(ctx, x, alpha):
        ctx.alpha = alpha

        return x.view_as(x)

    @staticmethod
    def backward(ctx, grad_output):
        output = grad_output.neg() * ctx.alpha

        return output, None


class DomainAdaptationModel(nn.Module):
    def __init__(self,embed_size= 512, hidden_size = 256, dropout=0.1):
        super(DomainAdaptationModel, self).__init__()



        self.dropout = nn.Dropout(dropout)

        self.domain_classifier = nn.Sequential(
            nn.Linear(embed_size, hidden_size),
            nn.Tanh(),
            nn.Linear(hidden_size, 2),
            # nn.LogSoftmax(dim=1),
        )


    def forward(
          self,
         pooled_output,
          grl_lambda = 1.0,
          ):
        pooled_output = self.dropout(pooled_output)


        reversed_pooled_output = GradientReversalFn.apply(pooled_output, grl_lambda)


        domain_pred = self.domain_classifier(reversed_pooled_output)

        return domain_pred.to(device)

class Disc_model(nn.Module):
    def __init__(self, hidden_size=256, input_size=512, dropout=0.1):
        super(Disc_model, self).__init__()
        self.linear = nn.Linear(input_size, hidden_size)
        self.tanh = nn.Tanh()
        self.fc = nn.Linear(hidden_size, 2)
        self.logsoftmax = nn.LogSoftmax(dim=1)
        self.dropout = nn.Dropout(dropout)
        
        
    def forward(self, enc_outputs, label ="trgt"):

        h = self.tanh(self.linear(self.dropout(enc_outputs)))#(N,hid_dim)
        z = self.fc(h) #(N,2)

        # prob = self.logsoftmax(z)


        return z
def train(train_dataloader, trgt_dataloader, valid_dataloader, encoder, decoder,classifier, n_epochs,
          print_every=100, plot_every=100):
    start = time.time()
    plot_losses = []
    print_loss_total = 0  # Reset every print_every
    plot_loss_total = 0  # Reset every plot_every
    total_domain_loss = 0
    total_nmt_loss =0
    criterion = nn.NLLLoss()
    best_valid_loss = 10
    early_stop_threshold = 3
    best_epoch = -1
    j = 0
    trgt_iter = iter(trgt_dataloader)
    
    
    for epoch in range(1, n_epochs + 1):
        total_loss = 0
        total_domain_loss = 0 
        total_nmt_loss = 0
        if j < early_stop_threshold:
            for data in train_dataloader:
                input_tensor, target_tensor = data
                inp_len = [len(input_tensor[i][input_tensor[i] != 0])
                           for i in range(input_tensor.shape[0])]
                trgt_len = [len(target_tensor[i][target_tensor[i] != 0])
                            for i in range(target_tensor.shape[0])]
                input_tensor = input_tensor[:, :max(inp_len)]
                target_tensor = target_tensor[:, :max(trgt_len)]

                encoder_optimizer.zero_grad()
                decoder_optimizer.zero_grad()

                encoder_outputs, encoder_hidden = encoder(input_tensor)
                decoder_outputs, _, _ = decoder(
                    encoder_outputs, encoder_hidden, target_tensor, MAX_LENGTH=max(trgt_len))
                context, _ = attn(encoder_hidden.permute(1,0,2),encoder_outputs )
                
                
                loss_nmt = criterion(
                    decoder_outputs.view(-1, decoder_outputs.size(-1)),
                    target_tensor.reshape(-1)
                )

                try:
                  da_inp,_ = next(trgt_iter)
                except Exception as e:
                  trgt_iter = iter(trgt_dataloader)
                  da_inp,_ = next(trgt_iter)


                trgt_da_len = [ len(da_inp[i][da_inp[i]!=0]) for i in range(da_inp.shape[0])]
                da_inp_tensor = da_inp[:, :max(trgt_da_len)].to(device) #(N,L)
                # da_inp_tensor = da_inp_tensor.T.to(device)
               
                encoder_outputs_trgt , encoder_hidden_trgt = encoder(da_inp_tensor)
                context_trgt, _ = attn(encoder_hidden_trgt.permute(1,0,2),encoder_outputs_trgt )
                
                
                y_s_domain = torch.zeros(input_tensor.shape[0], dtype=torch.long).to(device)

                domain_optimizer.zero_grad()
                y_t_domain = torch.ones(da_inp.shape[0], dtype=torch.long).to(device)
                domain_out = torch.cat((context.squeeze(1), context_trgt.squeeze(1)))
                y_domain = torch.cat((y_s_domain, y_t_domain))
                domain_pred = classifier(domain_out)

                lprobs = F.log_softmax(domain_pred, dim=-1, dtype=torch.float32)
                lprobs = lprobs.view(-1, lprobs.size(-1))
                loss_domain = F.nll_loss(lprobs, y_domain.view(-1))
                
                
                loss = loss_domain + loss_nmt
                
                total_domain_loss += loss_domain
                total_nmt_loss += loss_nmt
                total_loss += loss.item()
                
                
                loss.backward()
                encoder_optimizer.step()
                decoder_optimizer.step()
                domain_optimizer.step()

            scheduler_enc.step(total_loss / len(train_dataloader))
            scheduler_dec.step(total_loss / len(train_dataloader))
            print_loss_total = total_loss / len(train_dataloader)
            plot_loss_total = total_loss / len(train_dataloader)
            print("domain loss is {} , NMT loss is {}".format(total_domain_loss /len(train_dataloader),
                                                              total_nmt_loss /len(train_dataloader) ))
            valid_total_loss = 0
            
            
        

            for data in valid_dataloader:
                input_tensor, target_tensor = data
                inp_len = [len(input_tensor[i][input_tensor[i] != 0])
                           for i in range(input_tensor.shape[0])]
                trgt_len = [len(target_tensor[i][target_tensor[i] != 0])
                            for i in range(target_tensor.shape[0])]
                input_tensor = input_tensor[:, :max(inp_len)]
                target_tensor = target_tensor[:, :max(trgt_len)]
                with torch.no_grad():
                    encoder_outputs, encoder_hidden = encoder(input_tensor)
                    decoder_outputs, _, _ = decoder(
                        encoder_outputs, encoder_hidden, target_tensor, MAX_LENGTH=max(trgt_len))
    
                loss = criterion(
                    decoder_outputs.view(-1, decoder_outputs.size(-1)),
                    target_tensor.reshape(-1)
                )
    
                valid_total_loss += loss.item()
            valid_total_loss = valid_total_loss / len(valid_dataloader)
            if best_valid_loss > valid_total_loss:
                best_valid_loss = valid_total_loss
                best_epoch = epoch
                j = 0
                print("Saving ...")
                save_checkpoit(f"./checkpoint_RNN_enc_ADC_{z}",
                               encoder, encoder_optimizer)
                save_checkpoit(f"./checkpoint_RNN_dec_ADC_{z}",
                               decoder, decoder_optimizer)
                
                save_checkpoit(f"./checkpoint_RNN_classifier_ADC_{z}",
                               classifier, domain_optimizer)
            else:
                j += 1
    
        else:
    
            break

        if epoch % print_every == 0:
            print_loss_avg = print_loss_total / print_every

            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),
                                         epoch, epoch / n_epochs * 100, print_loss_avg))
            print("valid loss: ", valid_total_loss)
            print_loss_total = 0
            valid_total_loss = 0

        if epoch % plot_every == 0:
            plot_loss_avg = plot_loss_total / plot_every
            plot_losses.append(plot_loss_avg)
            plot_loss_total = 0

hidden_size = 512
batch_size = 16
learning_rate = 0.0001

input_lang, output_lang, train_dataloader = get_dataloader(batch_size)
_, _, valid_dataloader = get_dataloader(batch_size, label="valid")
_,_, trgt_dataloader = get_dataloader(batch_size, label="trgt")



for z in [0,1]:
    
    encoder = EncoderRNN(tokenizer.vocab_size, hidden_size).to(device)
    decoder = AttnDecoderRNN(hidden_size, tokenizer.vocab_size).to(device)
    attn = BahdanauAttention(hidden_size).to(device)
    
    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)
    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)
    scheduler_enc = ReduceLROnPlateau(
        encoder_optimizer, patience=2, factor=0.1, verbose=True)
    scheduler_dec = ReduceLROnPlateau(
        decoder_optimizer, patience=2, factor=0.1, verbose=True)
    torch.cuda.empty_cache()
    
    classifier = DomainAdaptationModel(512, 256).to(device) #ADC model
    # classifier = Disc_model(hidden_size = 256, input_size =512 ).to(device)
    domain_optimizer = optim.Adam(classifier.parameters(), lr=0.0001)
    # load_checkpoit(checkpoint_file, model, optimizer)
    
    Load= True
    if Load:
          load_checkpoit(f"./checkpoint_enc_base_{z}", encoder, encoder_optimizer)
          load_checkpoit(f"./checkpoint_dec_base_{z}", decoder, decoder_optimizer)    
    
    from transformers import AutoModelForSeq2SeqLM
    from peft import LoraModel, LoraConfig
    
    torch.cuda.empty_cache()
    
    config_enc = LoraConfig(
        task_type="SEQ_2_SEQ_LM",
        r= 64,
        # lora_alpha= 2,
        target_modules=["embedding"],
        lora_dropout=0.01,
    )
    
    lora_enc = LoraModel(encoder, config_enc, "default")
    encoder_optimizer = optim.Adam(lora_enc.parameters(), lr=learning_rate)
    
    # print(lora_enc)
    config_dec = LoraConfig(
        task_type="SEQ_2_SEQ_LM",
        r= 64,
        # lora_alpha= 2,
        target_modules=["embedding", "Wa", "Ua", "Va", "out"],
        lora_dropout=0.01,
    )
    
    lora_dec = LoraModel(decoder, config_dec, "default")
    decoder_optimizer = optim.Adam(lora_dec.parameters(), lr=learning_rate)
    # print(decoder)      
    train(train_dataloader, trgt_dataloader,valid_dataloader, lora_enc,lora_dec, classifier, 60, print_every=1, plot_every=5)
# model trained already on 60 epochs.

# save = False
# if save:
#     save_checkpoit("./Models/checkpoint-enc-arabert-MAG",encoder, encoder_optimizer)
#     save_checkpoit("./Models/checkpoint-dec-arabert-MAG",decoder, decoder_optimizer)

# if save:
#     save_checkpoit("./Models/checkpoint-enc-arabert-MAG", encoder, encoder_optimizer)
#     save_checkpoit("./Models/checkpoint-dec-arabert-MAG", decoder, decoder_optimizer)
from nltk.translate.bleu_score import corpus_bleu
Load= False
# if Load:
#       load_checkpoit("./checkpoint_RNN_enc_ADC", lora_enc, encoder_optimizer)
#       load_checkpoit("./checkpoint_RNN_dec_ADC", lora_dec, decoder_optimizer)    
      
      
# model_path_list = [("./checkpoint_RNN_enc_ADC", "./checkpoint_RNN_dec_ADC")]
for z in [0,1]:
    model_path_list = [(f"./checkpoint_RNN_enc_ADC_{z}", f"./checkpoint_RNN_dec_ADC_{z}")]
    domain = ["Gulf","Iraqi", "Levantine", "Nile_Basin", "North_Africa" ]
    # data_test_Nile_Basin = pd.read_csv("./data_Nile_Basin_preprocessed_test", sep="\t")
    bleu={"Gulf": [],
          "Iraqi": [],
          "Levantine":[],
          "Nile_Basin":[],
          "North_Africa":[]}
    data_length={ "Gulf": [],
          "Iraqi": [],
          "Levantine":[],
          "Nile_Basin":[],
          "North_Africa":[]}
    
    for model_path in model_path_list:
    
        checkpoint = torch.load(model_path[0])
        lora_enc.load_state_dict(checkpoint['model_state_dict'])
        checkpoint = torch.load(model_path[1])
        lora_dec.load_state_dict(checkpoint['model_state_dict'])
        for i, (k, v) in enumerate(bleu.items()):
            data = data_test[i]
            targets, outputs= evaluateRandomly(lora_enc, lora_dec,input_lang,output_lang, data)
            src_test = corpus_bleu(targets, outputs)
            data_length[k].append((outputs, targets))
            bleu[k].append(src_test)   
            print(bleu)        

        
# DC
# z=0, {'Gulf': [0.5310224111073124], 'Iraqi': [0.6592622223539577], 'Levantine': [0.48368281479827807], 'Nile_Basin': [0.532070243812256], 'North_Africa': [0.28964834517984506]}
# z=1, {'Gulf': [0.5294442655099446], 'Iraqi': [0.6424874290900522], 'Levantine': [0.4839757221505797], 'Nile_Basin': [0.5325088826979443], 'North_Africa': [0.2824505497853587]}

# z=0, {'Gulf': [0.5286421345864184], 'Iraqi': [0.6451238035757665], 'Levantine': [0.4797547896032244], 'Nile_Basin': [0.5287306304427326], 'North_Africa': [0.3031009057269515]}
# z=1, {'Gulf': [0.5230324680813325], 'Iraqi': [0.6375248370895716], 'Levantine': [0.4794644017220442], 'Nile_Basin': [0.5246593151671146], 'North_Africa': [0.30448544504128433]}
# bleu        
     
# print(data_length["Gulf"])

# for l in data_length["Gulf"]:
#     x,y = l
#     print(x[0], y[0])
# length_dict= {"Gulf": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []},
#               "Iraqi": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []} ,
#               "Levantine": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []} ,
#               "Nile_Basin": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []} ,
#               "North_Africa": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []} }
# for idx, (k,v) in enumerate(data_length.items()):
#     outputs, targets = data_length[k][0]
#     print(len(targets),k)
#     for i in range(len(targets)):
        
#         sent_len = len(tokenizer.tokenize(data_test[idx].source_lang[i]))
#         if sent_len <= 5:
#             length_dict[k]["<5"] += [(outputs[i],targets[i] )]
#         if 5<sent_len <= 10:
#             length_dict[k]["(5-10)"] += [(outputs[i],targets[i] )]
#         if 10<sent_len <= 15:
#             length_dict[k]["(10-15)"] += [(outputs[i],targets[i] )]
#         if 15<sent_len <= 20:
#             length_dict[k]["(15-20)"] += [(outputs[i],targets[i] )]
#         if 20 <sent_len <= 25:
#             length_dict[k]["(20-25)"] += [(outputs[i],targets[i] )]
#         if sent_len > 25:
#             length_dict[k][">25"] += [(outputs[i],targets[i] )]
# bleu_dict= {"Gulf": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []},
#               "Iraqi": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []} ,
#               "Levantine": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []} ,
#               "Nile_Basin": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []} ,
#               "North_Africa": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []} }     


# length_distribution= {"Gulf": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []},
#               "Iraqi": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []} ,
#               "Levantine": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []} ,
#               "Nile_Basin": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []} ,
#               "North_Africa": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []} }     

# for k,v in length_dict.items():
#     for k_2, v_2 in v.items():
#         outs = []
#         trgts =[]
#         for outputs, targets in length_dict[k][k_2]:
#             outs.append(outputs)
#             trgts.append(targets)
#         print(len(outs), len(trgts))
#         length_distribution[k][k_2].append(len(outs))
#         src_test = corpus_bleu(trgts, outs)
#         bleu_dict[k][k_2].append(src_test)
# bleu_dict
# len(length_dict["Gulf"]["<5"])


# bleu_domains= {"source": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []},
#               "Target": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []}}

# length_domains= {"source": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []},
#               "Target": {"<5": [], "(5-10)": [], "(10-15)": [], "(15-20)": [] , "(20-25)": [], ">25" : []}}


# for k,v in bleu_dict.items():
    
#     for k_2, v_2 in v.items():
        
#         if k != "North_Africa":
            
#             bleu_domains["source"][k_2] += v_2
#         else:
#             bleu_domains["Target"][k_2] += v_2

# for k_2, v_2 in bleu_domains["source"].items():
#     bleu_domains["source"][k_2] = np.array(bleu_domains["source"][k_2]).mean()


# for k,v in length_distribution.items():
    
#     for k_2, v_2 in v.items():
        
#         if k != "North_Africa":
            
#             length_domains["source"][k_2] += v_2
#         else:
#             length_domains["Target"][k_2] += v_2

# for k_2, v_2 in length_domains["source"].items():
#     length_domains["source"][k_2] = np.array(length_domains["source"][k_2]).sum()


# length_domains
# bleu_domains 
# x,y = data_length["Gulf"][0]
# print(len(y))
# for i, (k, v) in enumerate(data_length.items()):
    
  
#     output, target = data_length[k][0]

#     data_save = pd.DataFrame(data = {"trgt_prediction": output, "target_lang" : [target[idx][0] for idx in range(len(target))], "source_lang":data_test[i].source_lang })
#     data_save.to_csv("./translated_RNN_DC_{}".format(k))
# data_save.target_lang[0]
# length_distribution   
# bleu_dict 
   
#RNN ADC    
# bleu_dict
# {'Gulf': {'<5': [0.42099457239625393],
#   '(5-10)': [0.5001187147801613],
#   '(10-15)': [0.5525924807627097],
#   '(15-20)': [0.5615462794518186],
#   '(20-25)': [0.5558321211819169],
#   '>25': [0.5261997047365271]},
#  'Iraqi': {'<5': [0.4807813636147158],
#   '(5-10)': [0.6013196877215454],
#   '(10-15)': [0.6614130587030072],
#   '(15-20)': [0.8006654399441058],
#   '(20-25)': [0.8358752655790822],
#   '>25': [0.803467492856154]},
#  'Levantine': {'<5': [0.35851720136195364],
#   '(5-10)': [0.4924243693323097],
#   '(10-15)': [0.4984072284831753],
#   '(15-20)': [0.45452436183287603],
#   '(20-25)': [0.3915652328630542],
#   '>25': [0.4712481724728354]},
#  'Nile_Basin': {'<5': [0.3806159700972997],
#   '(5-10)': [0.5049132370443161],
#   '(10-15)': [0.5384186204649614],
#   '(15-20)': [0.5754364091265155],
#   '(20-25)': [0.5770260220324782],
#   '>25': [0.5896020538534049]},
#  'North_Africa': {'<5': [0.2155216286097322],
#   '(5-10)': [0.2865681710173671],
#   '(10-15)': [0.32043788085846636],
#   '(15-20)': [0.33027777261884944],
#   '(20-25)': [0.30754181715743617],
#   '>25': [0.3455072321530017]}}

#bleu per domain
# {'source': {'<5': 0.4102272768675558,
#   '(5-10)': 0.5246940022195832,
#   '(10-15)': 0.5627078471034633,
#   '(15-20)': 0.598043122588829,
#   '(20-25)': 0.5900746604141328,
#   '>25': 0.5976293559797303},
#  'Target': {'<5': [0.2155216286097322],
#   '(5-10)': [0.2865681710173671],
#   '(10-15)': [0.32043788085846636],
#   '(15-20)': [0.33027777261884944],
#   '(20-25)': [0.30754181715743617],
#   '>25': [0.3455072321530017]}}



#RNN BASE    
#bleu per dialect
# {'Gulf': {'<5': [0.4300501761659375],
#   '(5-10)': [0.49256198824191355],
#   '(10-15)': [0.5478683228215929],
#   '(15-20)': [0.5862079381437127],
#   '(20-25)': [0.59127150211832],
#   '>25': [0.558076658474292]},
#  'Iraqi': {'<5': [0.4575358295691908],
#   '(5-10)': [0.5831723715296799],
#   '(10-15)': [0.6833118343077081],
#   '(15-20)': [0.7553845807777246],
#   '(20-25)': [0.8081787395233873],
#   '>25': [0.7103603102125331]},
#  'Levantine': {'<5': [0.36342158554788406],
#   '(5-10)': [0.4864761289905619],
#   '(10-15)': [0.5052615965265378],
#   '(15-20)': [0.4550760938820954],
#   '(20-25)': [0.3865762172352586],
#   '>25': [0.47029960509809615]},
#  'Nile_Basin': {'<5': [0.3822746307907342],
#   '(5-10)': [0.5047273160196256],
#   '(10-15)': [0.523780054382354],
#   '(15-20)': [0.5763686033964004],
#   '(20-25)': [0.56822056145893],
#   '>25': [0.6035073732501369]},
#  'North_Africa': {'<5': [0.20749129139258932],
#   '(5-10)': [0.2624447720426182],
#   '(10-15)': [0.289845500782314],
#   '(15-20)': [0.31582448304276783],
#   '(20-25)': [0.29428214161597427],
#   '>25': [0.35209957597164426]}}

#bleu per domain
# {'source': {'<5': 0.4083205555184366,
#   '(5-10)': 0.5167344511954453,
#   '(10-15)': 0.5650554520095482,
#   '(15-20)': 0.5932593040499833,
#   '(20-25)': 0.5885617550839739,
#   '>25': 0.5855609867587646},
#  'Target': {'<5': [0.20749129139258932],
#   '(5-10)': [0.2624447720426182],
#   '(10-15)': [0.289845500782314],
#   '(15-20)': [0.31582448304276783],
#   '(20-25)': [0.29428214161597427],
#   '>25': [0.35209957597164426]}}

# RNN DC
# Bleu per dialect
# {'Gulf': {'<5': [0.4202736420318076],
#   '(5-10)': [0.4993111079411072],
#   '(10-15)': [0.5519339097151005],
#   '(15-20)': [0.582755377082742],
#   '(20-25)': [0.5848126005915463],
#   '>25': [0.5741043050450261]},
#  'Iraqi': {'<5': [0.47069651034676147],
#   '(5-10)': [0.5931609334753333],
#   '(10-15)': [0.6870024494200527],
#   '(15-20)': [0.8137733598199054],
#   '(20-25)': [0.8928075568557412],
#   '>25': [0.7403412853078745]},
#  'Levantine': {'<5': [0.3591337293112866],
#   '(5-10)': [0.4824561859384209],
#   '(10-15)': [0.504215455757554],
#   '(15-20)': [0.46086834287283895],
#   '(20-25)': [0.39198369667322286],
#   '>25': [0.4642073952882502]},
#  'Nile_Basin': {'<5': [0.37525711215393537],
#   '(5-10)': [0.5045841338471609],
#   '(10-15)': [0.5401486478699992],
#   '(15-20)': [0.5775083396008608],
#   '(20-25)': [0.5503975663711396],
#   '>25': [0.6345055473359267]},
#  'North_Africa': {'<5': [0.2057844349354044],
#   '(5-10)': [0.26414655711784174],
#   '(10-15)': [0.2931127108164214],
#   '(15-20)': [0.30910312912878574],
#   '(20-25)': [0.276689837655692],
#   '>25': [0.3469864125033848]}}

# {'source': {'<5': 0.4063402484609478,
#   '(5-10)': 0.5198780903005056,
#   '(10-15)': 0.5708251156906766,
#   '(15-20)': 0.6087263548440868,
#   '(20-25)': 0.6050003551229125,
#   '>25': 0.6032896332442693},
#  'Target': {'<5': [0.2057844349354044],
#   '(5-10)': [0.26414655711784174],
#   '(10-15)': [0.2931127108164214],
#   '(15-20)': [0.30910312912878574],
#   '(20-25)': [0.276689837655692],
#   '>25': [0.3469864125033848]}}




   
# targets, outputs= evaluateRandomly(lora_enc, lora_dec,input_lang,output_lang, data_test[4])
# src_test = corpus_bleu(targets, outputs )
# print(src_test)

# encoder = EncoderRNN(tokenizer.vocab_size, hidden_size).to(device)
# decoder = AttnDecoderRNN(hidden_size, tokenizer.vocab_size).to(device)
# attn = BahdanauAttention(hidden_size).to(device)

# encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)
# decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)
# scheduler_enc = ReduceLROnPlateau(
#     encoder_optimizer, patience=2, factor=0.1, verbose=True)
# scheduler_dec = ReduceLROnPlateau(
#     decoder_optimizer, patience=2, factor=0.1, verbose=True)
# torch.cuda.empty_cache()

# # classifier = DomainAdaptationModel(512, 256).to(device) #ADC model
# classifier = Disc_model(hidden_size = 256, input_size =512 ).to(device)
# domain_optimizer = optim.Adam(classifier.parameters(), lr=0.0001)

# from nltk.translate.bleu_score import corpus_bleu

# model_path_list = [("./checkpoint_enc_base", "./checkpoint_dec_base")]
# domain = ["Gulf","Iraqi", "Levantine", "Nile_Basin", "North_Africa" ]
# # data_test_Nile_Basin = pd.read_csv("./data_Nile_Basin_preprocessed_test", sep="\t")
# bleu={"Gulf": [],
#       "Iraqi": [],
#       "Levantine":[],
#       "Nile_Basin":[],
#       "North_Africa":[]}

# for model_path in model_path_list:

#   checkpoint = torch.load(model_path[0])
#   encoder.load_state_dict(checkpoint['model_state_dict'])
#   checkpoint = torch.load(model_path[1])
#   decoder.load_state_dict(checkpoint['model_state_dict'])
#   for _ in range(2):
#       for i, (k, v) in enumerate(bleu.items()):
#           data = data_test[i]
#           targets, outputs= evaluateRandomly(encoder, decoder,input_lang,output_lang, data)
#           src_test = corpus_bleu(targets, outputs)
#           bleu[k].append(src_test)
#           print(bleu)










# RNN [BASE] results
# {'Gulf': [0.522964417478443], 'Iraqi': [0.6384527886439714], 
# 'Levantine': [0.4735739191630386], 'Nile_Basin': [0.5224566364788962], 
# 'North_Africa': [0.28472152417180013]}

# %RNN [ADC, DC] results
# % {'Gulf': [0.5153496529603953, 0.5197133579054222], 
# 'Iraqi': [0.6429561026081463, 0.6554968268934772], 
# 'Levantine': [0.470284760930885, 0.4725329516224297], 
# 'Nile_Basin': [0.5190594189232228, 0.5203003294206927], 
# 'North_Africa': [0.3021263956156627, 0.287249049949042]}


# %RNN [ADC,ADC,DC, DC] results
# {'Gulf': [0.51430250740367, 0.5188333193281788, 0.5170137730898926, 0.5183603423104705], 'Iraqi': [0.6462637125888295, 0.6458788031720968, 0.6517693335186554, 0.6488816183306341], 'Levantine': [0.46909941886284084, 0.469635622780365, 0.4735585425316672, 0.47525309526625936], 'Nile_Basin': [0.515200308659568, 0.5185069682831277, 0.5216498629322388, 0.5238914520853291], 'North_Africa': [0.30710170023048955, 0.3049681142719104, 0.2852266585719229, 0.28594945441959224]}

# RNN [BASE, BASE] results
# {'Gulf': [0.5185840672864156, 0.5188019417781918], 'Iraqi': [0.6494339215321003, 0.6442295865885873], 'Levantine': [0.47230651988758127, 0.4723941582732244], 'Nile_Basin': [0.5221850151735339, 0.5173570907480141], 'North_Africa': [0.2856997197208062, 0.28529903906484255]}

#running DC 




# Load= True
# if Load:
#       load_checkpoit("./checkpoint_RNN_enc_DC", lora_enc, encoder_optimizer)
#       load_checkpoit("./checkpoint_RNN_dec_DC", lora_dec, decoder_optimizer)    

# from nltk.translate.bleu_score import corpus_bleu
# targets, outputs= evaluateRandomly(encoder, decoder,input_lang,output_lang, data_test[4])
# src_test = corpus_bleu(targets, outputs )
# print(src_test)


def evaluate(encoder, decoder, sentence, input_lang, output_lang):
    with torch.no_grad():
        sentence = normalizeString(sentence)
        input_tensor = tensorFromSentence(input_lang, sentence)

        encoder_outputs, encoder_hidden = encoder(input_tensor)
        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)

        _, topi = decoder_outputs.squeeze().topk(1)
        decoded_ids = topi.squeeze()

        decoded_words = []
        for idx in decoded_ids:
            if idx.item() == EOS_token:
                decoded_words.append('<EOS>')
                break
            decoded_words.append(output_lang.index2word.get(idx.item(), tokenizer.unk_token))
    return decoded_words, decoder_attn
# output_lang.index2word.get(word, tokenizer.unk_token)

import matplotlib.pyplot as plt
plt.switch_backend('agg')
plt.close('agg')
import matplotlib.ticker as ticker
import numpy as np

def showAttention(input_sentence, output_words, attentions):
    fig = plt.figure()
    ax = fig.add_subplot(111)
    cax = ax.matshow(attentions.cpu().numpy(), cmap='hot')
    fig.colorbar(cax)

    # Set up axes
    ax.set_xticklabels([''] + tokenizer.tokenize(input_sentence) +
                       ['<EOS>'], rotation=90)
    ax.set_yticklabels([''] + output_words)

    # Show label at every tick
    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))
    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))
    plt.savefig("./mygraph.png")
    print(plt.show())

def evaluateAndShowAttention(input_sentence):
    output_words, attentions = evaluate(lora_enc, lora_dec,input_sentence,input_lang,output_lang)
    output_sentence = tokenizer.convert_tokens_to_string(output_words)
    output_sentence = output_sentence.replace("[SEP]", "")
    output_sentence = output_sentence.replace("[PAD]", "")
    output_sentence = output_sentence.replace("[CLS]", "")
    output_sentence = output_sentence.replace("[UNK]", "")
    print(output_words)
    print('input =', input_sentence)
    print('output =', output_sentence)
    print(attentions[0, :len(output_words), :].cpu().numpy())
    showAttention(input_sentence, output_words, attentions[0, :len(output_words), :])

# evaluateAndShowAttention(data_test[0].source_lang[999])

# print(data_test[0].source_lang[:1000])
# 0.3020224287635638, ADC , r=64 , ["embedding", "Wa", "Ua", "Va", "out"] in decoder, ["embedding"] in encoder
# 0.284810669463694, Base model
# 0.2842807724372342 DC, r=64



# 0.28317893877172307 r=128, DC 



# ADC training
# domain loss is 0.5811155438423157 , NMT loss is 0.2654782235622406
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# Saving checkpoint...
# 4m 6s (- 242m 44s) (1 1%) 0.8466
# valid loss:  2.250013959505519
# domain loss is 1.187972903251648 , NMT loss is 0.5272190570831299
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# Saving checkpoint...
# 8m 14s (- 238m 51s) (2 3%) 0.8686
# valid loss:  2.222963709029995
# domain loss is 1.787545919418335 , NMT loss is 0.7862391471862793
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# Saving checkpoint...
# 12m 20s (- 234m 31s) (3 5%) 0.8586
# valid loss:  2.197523628785962
# domain loss is 2.3780722618103027 , NMT loss is 1.043944001197815
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# Saving checkpoint...
# 16m 30s (- 231m 5s) (4 6%) 0.8482
# valid loss:  2.16138528702689
# domain loss is 2.9632105827331543 , NMT loss is 1.301742672920227
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# Saving checkpoint...
# 20m 47s (- 228m 38s) (5 8%) 0.8429
# valid loss:  2.1510355506764083
# domain loss is 3.543869972229004 , NMT loss is 1.5577654838562012
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# Saving checkpoint...
# 25m 6s (- 225m 57s) (6 10%) 0.8367
# valid loss:  2.1432292158486415
# domain loss is 4.1205244064331055 , NMT loss is 1.817897081375122
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# Saving checkpoint...
# 29m 24s (- 222m 43s) (7 11%) 0.8368
# valid loss:  2.1398876270309826
# domain loss is 4.693430423736572 , NMT loss is 2.0710792541503906
# 33m 41s (- 218m 59s) (8 13%) 0.8261
# valid loss:  2.1564481638494084
# domain loss is 5.270640850067139 , NMT loss is 2.33014178276062
# 37m 56s (- 214m 59s) (9 15%) 0.8363
# valid loss:  2.1423113895244286
# domain loss is 5.848741054534912 , NMT loss is 2.587688684463501
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# Saving checkpoint...
# 42m 12s (- 211m 4s) (10 16%) 0.8356
# valid loss:  2.117885705095823
# domain loss is 6.428314685821533 , NMT loss is 2.8426811695098877
# 46m 27s (- 206m 57s) (11 18%) 0.8346
# valid loss:  2.1516995221864983
# domain loss is 7.006335258483887 , NMT loss is 3.0985701084136963
# 50m 43s (- 202m 52s) (12 20%) 0.8339
# valid loss:  2.1228282654871706
# domain loss is 7.583375930786133 , NMT loss is 3.352560520172119
# 55m 0s (- 198m 52s) (13 21%) 0.8310
# valid loss:  2.137062135485352
# Loading checkpoint...



# 0.2854881701564368 North Africa

# targets_trgt, outputs_trgt= evaluateRandomly(encoder, decoder,input_lang,output_lang, data_trgt_test)
# trgt_test = corpus_bleu(targets_trgt, outputs_trgt )

# print("Bley score for target domain is {} and for source domain is {}".format(
#     trgt_test, src_test))

# 20m 51s (- 396m 20s) (3 5%) 2.5290
# valid loss:  2.53274218434269
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# 27m 46s (- 388m 57s) (4 6%) 2.3451
# valid loss:  2.380791461294137
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# 34m 40s (- 381m 21s) (5 8%) 2.1563
# valid loss:  2.2349102672153305
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# 41m 35s (- 374m 15s) (6 10%) 1.9761
# valid loss:  2.0933535480962218
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# 48m 31s (- 367m 25s) (7 11%) 1.8074
# valid loss:  1.9779538565035004
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# 55m 29s (- 360m 39s) (8 13%) 1.6417
# valid loss:  1.8644170533250837
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# 62m 27s (- 353m 58s) (9 15%) 1.5181
# valid loss:  1.7763019984642279
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# 69m 27s (- 347m 16s) (10 16%) 1.4052
# valid loss:  1.7192494448961564
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# 76m 25s (- 340m 26s) (11 18%) 1.2940
# valid loss:  1.6425226337267358
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# 83m 22s (- 333m 31s) (12 20%) 1.2105
# valid loss:  1.6011189689624656
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# 90m 18s (- 326m 29s) (13 21%) 1.1238
# valid loss:  1.5529060208971062
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# 97m 16s (- 319m 36s) (14 23%) 1.0461
# valid loss:  1.5181382208773233
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# 104m 14s (- 312m 44s) (15 25%) 0.9748
# valid loss:  1.4767674151265506
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# 111m 18s (- 306m 6s) (16 26%) 0.9130
# valid loss:  1.4496185469974592
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# 118m 15s (- 299m 7s) (17 28%) 0.8473
# valid loss:  1.44424220937693
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# 125m 19s (- 292m 26s) (18 30%) 0.7839
# valid loss:  1.4260231621780441
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# 132m 15s (- 285m 22s) (19 31%) 0.7357
# valid loss:  1.405092654225317
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# 139m 13s (- 278m 26s) (20 33%) 0.6760
# valid loss:  1.3993631177627057
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# 146m 9s (- 271m 25s) (21 35%) 0.6304
# valid loss:  1.360076682696354
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# 153m 3s (- 264m 23s) (22 36%) 0.5821
# valid loss:  1.3508746263399287
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# 160m 2s (- 257m 26s) (23 38%) 0.5364
# valid loss:  1.3439355070874528
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# 167m 0s (- 250m 30s) (24 40%) 0.4979
# valid loss:  1.342866928000184
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# 174m 7s (- 243m 46s) (25 41%) 0.4622
# valid loss:  1.337723237941566
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# 181m 5s (- 236m 48s) (26 43%) 0.4214
# valid loss:  1.331753376979851
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# 188m 2s (- 229m 49s) (27 45%) 0.3846
# valid loss:  1.3081053505534108
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# 194m 58s (- 222m 49s) (28 46%) 0.3548
# valid loss:  1.3024696555820483
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# 201m 54s (- 215m 49s) (29 48%) 0.3247
# valid loss:  1.3016445344693741
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# 208m 52s (- 208m 52s) (30 50%) 0.2938
# valid loss:  1.282628129534785
# 215m 47s (- 201m 52s) (31 51%) 0.2677
# valid loss:  1.3049773452351394
# 222m 45s (- 194m 54s) (32 53%) 0.2460
# valid loss:  1.289009770505058
# Saving ...
# Saving checkpoint...
# Saving checkpoint...
# 229m 52s (- 188m 4s) (33 55%) 0.2200
# valid loss:  1.2718950749817983
# 236m 54s (- 181m 9s) (34 56%) 0.1990
# valid loss:  1.2765962724546784
# 243m 55s (- 174m 13s) (35 58%) 0.1785
# valid loss:  1.2735043300261486