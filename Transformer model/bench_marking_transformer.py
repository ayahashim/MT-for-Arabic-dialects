# -*- coding: utf-8 -*-
"""Bench_marking_transformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jp72iWn8gH36tYhFmssmJtM29gJ7w3Zl
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/My Drive"

# !pip install torch
#
import torch

# -*- coding: utf-8 -*-
"""Transformer_batch_size_arabert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yKjYDLWxlvcf-zuNLSXj0c8JgvfOD2ey
"""

from io import open
import unicodedata
import re
import random
import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F
from transformers import ( AutoModel, AutoTokenizer)
import numpy as np
from torch.utils.data import TensorDataset, DataLoader, RandomSampler
from torch import Tensor
import torch
import torch.nn as nn
from torch.nn import Transformer
import math
from timeit import default_timer as timer
from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts, CosineAnnealingLR

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

import pandas as pd
from sklearn.model_selection import train_test_split
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_name = "aubmindlab/bert-base-arabertv02-twitter"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = AutoTokenizer.from_pretrained(model_name)





# data_vocab = './data_vocab'
# # data_train_file = "./data_Nile_Basin_preprocessed_train"
# data_valid_file = "data_Nile_Basin_preprocessed_valid"
# data_valid= pd.read_csv("./data_Nile_Basin_preprocessed_valid", sep="\t")
# data_test= pd.read_csv("./data_Nile_Basin_preprocessed_test", sep="\t")
SOS_token = tokenizer.cls_token_id
EOS_token = tokenizer.sep_token_id
PAD_token = tokenizer.pad_token_id


domain = ["Gulf","Iraqi", "Levantine", "Nile_Basin", "North_Africa" ]

data_train =[]
data_test = []
for i in domain:
    data_train_1 = pd.read_csv("./{}_train_benchmark".format(i), sep ="\t")
    data_test_1 = pd.read_csv("./{}_test_benchmark".format(i), sep ="\t")
    data_train.append(data_train_1)
    data_test.append(data_test_1)
    
data_all_1 = pd.concat([data_train[0], data_train[1]], ignore_index = True)
data_all_2 = pd.concat([data_train[2], data_train[3]], ignore_index = True)
data_all_3 = pd.concat([data_all_2, data_all_1], ignore_index = True)
data_train_all = pd.concat([data_all_3, data_train[4]], ignore_index = True)


data_all_1 = pd.concat([data_test[0], data_test[1]], ignore_index = True)
data_all_2 = pd.concat([data_test[2], data_test[3]], ignore_index = True)
data_all_3 = pd.concat([data_all_2, data_all_1], ignore_index = True)
data_test_all = pd.concat([data_all_3, data_test[4]], ignore_index = True)



data = pd.read_csv("./data_domian_cosine_arabert_benchmark_all", sep ="\t")
data_train_1 =[]



for i in range(5):
    data_train_1.append(data.sort_values("score_{}".format(domain[i]), ascending = False))
    data_train_1[i] = data_train_1[i][["source_lang_{}".format(domain[i]), "target_lang"]]
    data_train_1[i].rename(columns = {"source_lang_{}".format(domain[i]):"source_lang"}, inplace = True)

j=2 #Levantine dialect model performance

data_vocab_all = pd.concat([data_train_all, data_test_all], ignore_index = True)


data_all_1 = pd.concat([data_train_1[0][:5000], data_train_1[1][:5000]], ignore_index = True)
data_all_2 = pd.concat([data_train_1[2][:5000], data_train_1[3][:5000]], ignore_index = True)
data_all_3 = pd.concat([data_all_2, data_all_1], ignore_index = True)
data_all_4 = pd.concat([data_all_3, data_train_1[4][:5000]], ignore_index = True)

# data_all_4.drop_duplicates(inplace = True)
# data_all_4.shape

data_train_all_BT = pd.concat([data_train_all, data_all_4], ignore_index = True)
# data_train_all_BT = pd.concat([data_train, data_train_1[j][:5000]], ignore_index = True)
# data_train_all_BT.shape


data_vocab_all_BT = pd.concat([data_vocab_all,data_all_4 ], ignore_index = True)
# data_vocab_all_BT.shape

# data_train_BT= data_train[i]
# print(data_train_BT.shape)


# for i in range(5):
#     data_train_1.append(data.sort_values("score_{}".format(domain[i]), ascending = False))


# data_train_all_BT

# -*- coding: utf-8 -*-
"""Transformer_MT_Base_NFKC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12F8ltvhlJKIn0v_2gDSjhBnUWa7QzDMx
"""

# !sudo echo -ne '\n' | sudo add-apt-repository ppa:alessandro-strada/ppa >/dev/null 2>&1 # note: >/dev/null 2>&1 is used to supress printing
# !sudo apt update >/dev/null 2>&1
# !sudo apt install google-drive-ocamlfuse >/dev/null 2>&1
# !google-drive-ocamlfuse
# !sudo apt-get install w3m >/dev/null 2>&1 # to act as web browser
# !xdg-settings set default-web-browser w3m.desktop >/dev/null 2>&1 # to set default browser
# %cd /content
# !mkdir gdrive
# %cd gdrive
# !mkdir "My Drive"
# !google-drive-ocamlfuse "/content/gdrive/My Drive"

# from google.colab import drive
# drive.mount('/content/drive')

# # !pip install transformers


# data_select = pd.read_csv("./data_selected_sent_arabert_2", sep="\t")
# data_select.shape

# data_train = pd.read_csv("./data_Nile_Basin_preprocessed_train", sep="\t")
# # data_select = pd.read_csv("./data_finetune_score_LORA", sep="\t")
# data_select = pd.read_csv("./data_selected_sent_arabert_2", sep="\t")

# data_select.dropna(inplace=True)
# for idx in data_select.index:
#   sent = data_select["target_lang"][idx]
#   print(sent,idx)
#   z= re.findall('\d+', sent)
#   if z:
#     data_select=data_select.drop([idx])
# data_train_2 = data_select.sort_values("score", ascending = False, inplace = True)
# data_train_2 =data_select[:5000]
# data_train_2 = data_train_2[["target_lang","source_lang"] ]
# data_train = data_train [["target_lang","source_lang"] ]
# data_train_2.shape

# df = pd.concat([data_train, data_train_2], ignore_index= True)
# df.head()

# df.to_csv("./data_Nile_basin_BT_sent_arabert_2", sep="\t")
# data_train_file = "./data_Nile_basin_BT_sent_arabert_2"

#determine i
# i= 2
# z= 2
# sent_bert data

# data = pd.read_csv("./data_selected_domain_finetune_arabert_large_MADAR_2", sep ="\t")



# data = pd.read_csv("./data_domian_cosine_sent_bert_all", sep ="\t")
# sent_arabert data trained on 5 epochs
# data = pd.read_csv("./data_domian_cosine_sent_arabert_large_all", sep ="\t")
# arabert data

# data_selected_domain_finetune_arabert_large
# data_selected_domain_finetune_arabert_large_all
# data.to_csv("./data_selected_domain_finetune_arabert_large_{}".format(domain[i]), sep ="\t")
# data.columns

# # sent_arabert data trained on 10 epochs
# data = pd.read_csv("./data_domian_cosine_sent_arabert_large_all_2", sep ="\t")

# domain finetune sorting
# data_1 = data.loc[data["prediction"]==1].sort_values("prob",ascending = False)
# data_2 = data.loc[data["prediction"]==0].sort_values("prob",ascending = True)

# data_finetune = pd.concat([data_1, data_2[:(20000 - data_1.shape[0])]])



# domain cosine sorting
# data_1 = data.sort_values("score_{}".format(domain[i]), ascending = False)

# data_1 = data_1[:20000]
# data_finetune = data_1[["source_lang_{}".format(domain[i]), "target_lang"]]
# data_finetune.rename(columns = {"source_lang_{}".format(domain[i]):"source_lang"}, inplace = True)
# data_train_BT = pd.concat([data_train[i], data_finetune], ignore_index = True)


class Lang:
    def __init__(self, name):
        self.name = name
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.word2index = { self.tokenizer.pad_token : self.tokenizer.pad_token_id}
        self.word2count = {}
        self.index2word = {self.tokenizer.pad_token_id: self.tokenizer.pad_token}
        self.n_words = 1 # Count PAD token

    def addSentence(self, sentence):
        for word in self.tokenizer.tokenize(sentence, add_special_tokens= True):
            self.addWord(word)

    def addWord(self, word):
        if word not in self.word2index:
            self.word2index[word] = self.tokenizer.convert_tokens_to_ids(word)
            self.word2count[word] = 1
            self.index2word[self.tokenizer.convert_tokens_to_ids(word)] = word
            self.n_words += 1
        else:
            self.word2count[word] += 1

# Turn a Unicode string to plain ASCII, thanks to
# https://stackoverflow.com/a/518232/2809427
def unicodeToAscii(s):
    return ''.join(
        c for c in unicodedata.normalize('NFKC', s)
        if unicodedata.category(c) != 'Mn'
    )

# Lowercase, trim, and remove non-letter characters


def normalizeString(s):
    s = unicodeToAscii(s.lower().strip())
    s = re.sub(r"([?.!,¿])", r" \1 ", s)
    s = re.sub(r'[" "]+', " ", s)

    s = re.sub(r"[^a-zA-Z؀-ۿ?.!,¿]+", " ", s)
    s = re.sub(r"([.!?])", r" \1", s)
    # s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)
    return s

def readLangs(lang1, lang2, reverse=False, label ="train"):
    print("Reading lines...")

    # Read the file and split into lines
    if label=="vocab":
        # lines = open(data_vocab, encoding='utf-8').\
        # read().strip().split('\n')
        data = data_vocab_all_BT
    if label =="train":
        # lines = open(data_train_file, encoding='utf-8').\
        # read().strip().split('\n')
        data = data_train_all_BT
    if label =="valid":
        # lines = open(data_valid_file, encoding='utf-8').\
        # read().strip().split('\n')
        data = data_valid_all
    # lines = open('/content/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\
    #     read().strip().split('\n')

    # Split every line into pairs and normalize
    pairs = [[normalizeString(data.source_lang[idx]), normalizeString(data.target_lang[idx])] for idx in data.index]

    # Reverse pairs, make Lang instances
    if reverse:
        pairs = [list(reversed(p)) for p in pairs]
        input_lang = Lang(lang2)
        output_lang = Lang(lang1)
    else:
        input_lang = Lang(lang1)
        output_lang = Lang(lang2)

    return input_lang, output_lang, pairs

MAX_LENGTH = 200
def prepareData(lang1, lang2, reverse=False, label="train"):
    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse, label = label)
    print("Read %s sentence pairs" % len(pairs))
    # pairs = filterPairs(pairs)
    # print("Trimmed to %s sentence pairs" % len(pairs))
    print("Counting words...")
    pairs = pairs[1:]
    for pair in pairs:
        input_lang.addSentence(pair[0])
        input_lang.addSentence(pair[1])
    output_lang = input_lang
    print("Counted words:")
    print(input_lang.name, input_lang.n_words)
    print(output_lang.name, output_lang.n_words)
    return input_lang, output_lang, pairs


# input_lang, output_lang, pairs = prepareData('Cairo', 'MSA')



def indexesFromSentence(lang, sentence):
    return [lang.word2index.get(word,0) for word in tokenizer.tokenize(sentence, add_special_tokens=True)]

def tensorFromSentence(lang, sentence):
    indexes = indexesFromSentence(lang, sentence)
    indexes.append(EOS_token)
    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)

def tensorsFromPair(pair):
    input_tensor = tensorFromSentence(input_lang, pair[0])
    target_tensor = tensorFromSentence(output_lang, pair[1])
    return (input_tensor, target_tensor)

def get_dataloader(batch_size, label ="train" ):
    input_lang, output_lang, _ = prepareData('ar', 'arz', label="vocab")
    _, _, pairs = prepareData('ar', 'arz', label = label)
    n = len(pairs)
    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)
    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)

    for idx, (inp, tgt) in enumerate(pairs):
        inp_ids = indexesFromSentence(input_lang, inp)
        tgt_ids = indexesFromSentence(output_lang, tgt)
        # inp_ids.append(EOS_token)
        # tgt_ids.append(EOS_token)
        input_ids[idx, :len(inp_ids)] = inp_ids
        target_ids[idx, :len(tgt_ids)] = tgt_ids

    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),
                               torch.LongTensor(target_ids).to(device))

    train_sampler = RandomSampler(train_data)
    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)
    return input_lang, output_lang, train_dataloader

# input_lang, output_lang, train_dataloader = get_dataloader(32)

# data = iter(train_dataloader)
# next(data)[0]

# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.
class PositionalEncoding(nn.Module):
    def __init__(self,
                 emb_size: int,
                 dropout: float,
                 maxlen: int = 5000):
        super(PositionalEncoding, self).__init__()
        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)
        pos = torch.arange(0, maxlen).reshape(maxlen, 1)
        pos_embedding = torch.zeros((maxlen, emb_size))
        pos_embedding[:, 0::2] = torch.sin(pos * den)
        pos_embedding[:, 1::2] = torch.cos(pos * den) # is an indexing operation that targets every other
                                                      #column of the pos_embedding tensor, starting from
                                                      #the second column (index 1) and going up to the last
                                                      #column (index emb_size-1).
        pos_embedding = pos_embedding.unsqueeze(-2)

        self.dropout = nn.Dropout(dropout)
        self.register_buffer('pos_embedding', pos_embedding)

    def forward(self, token_embedding: Tensor):
        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])

# helper Module to convert tensor of input indices into corresponding tensor of token embeddings
class TokenEmbedding(nn.Module):
    def __init__(self, vocab_size: int, emb_size):
        super(TokenEmbedding, self).__init__()
        self.embedding = nn.Embedding(vocab_size, emb_size)
        self.emb_size = emb_size

    def forward(self, tokens: Tensor):
        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)

# Seq2Seq Network
class Seq2SeqTransformer(nn.Module):
    def __init__(self,
                 num_encoder_layers: int,
                 num_decoder_layers: int,
                 emb_size: int,
                 nhead: int,
                 src_vocab_size: int,
                 tgt_vocab_size: int,
                 dim_feedforward: int = 512,
                 dropout: float = 0.1):
        super(Seq2SeqTransformer, self).__init__()
        self.transformer = Transformer(d_model=emb_size,
                                       nhead=nhead,
                                       num_encoder_layers=num_encoder_layers,
                                       num_decoder_layers=num_decoder_layers,
                                       dim_feedforward=dim_feedforward,
                                       dropout=dropout)
        self.generator = nn.Linear(emb_size, tgt_vocab_size)

        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)
        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)
        self.positional_encoding = PositionalEncoding(
            emb_size, dropout=dropout)

    def forward(self,
                src: Tensor,
                trg: Tensor,
                src_mask: Tensor,
                tgt_mask: Tensor,
                src_padding_mask: Tensor,
                tgt_padding_mask: Tensor,
                memory_key_padding_mask: Tensor):
        src_emb = self.positional_encoding(self.src_tok_emb(src))
        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))
        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,
                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)
        return self.generator(outs)

    def encode(self, src: Tensor, src_mask: Tensor, src_padding_mask):
        return self.transformer.encoder(self.positional_encoding(
                            self.src_tok_emb(src)), src_mask, src_key_padding_mask  = src_padding_mask)

    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor, tgt_padding_mask):
        return self.transformer.decoder(self.positional_encoding(
                          self.tgt_tok_emb(tgt)), memory,
                          tgt_mask, tgt_key_padding_mask = tgt_padding_mask)

def generate_square_subsequent_mask(sz):
    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)
    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
    return mask

PAD_IDX = 0
def create_mask(src, tgt):
    src_seq_len = src.shape[0]
    tgt_seq_len = tgt.shape[0]

    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)
    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)

    src_padding_mask = (src == PAD_IDX).transpose(0, 1)
    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)
    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask

def train_epoch(model, optimizer ):
    model.train()
    losses = 0
    valid_losses =0

    start_time = timer()
    for src, tgt in train_dataloader:
        src = src.to(device)
        tgt = tgt.to(device)

        inp_len = [ len(src[i][src[i]!=0]) for i in range(src.shape[0])]
        trgt_len = [len(tgt[i][tgt[i]!=0]) for i in range(tgt.shape[0]) ]
        input_tensor = src[:, :max(inp_len)]
        target_tensor = tgt[:, :max(trgt_len)]
        src = input_tensor.T.to(device)
        tgt = target_tensor.T.to(device)
        tgt_input = tgt[:-1, :]

        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)

        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)

        optimizer.zero_grad()

        tgt_out = tgt[1:, :]
        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))
        loss.backward()

        optimizer.step()
        # scheduler.step()
        losses += loss.item()
    end_time = timer()
    # model.eval()
    # valid_start_time = timer()
    # for src, tgt in valid_dataloader:
    #     # Move batch to device
    #     src = src.to(device)
    #     tgt = tgt.to(device)

    #     inp_len = [ len(src[i][src[i]!=0]) for i in range(src.shape[0])]
    #     trgt_len = [len(tgt[i][tgt[i]!=0]) for i in range(tgt.shape[0]) ]
    #     input_tensor = src[:, :max(inp_len)]
    #     target_tensor = tgt[:, :max(trgt_len)]
    #     src = input_tensor.T.to(device)
    #     tgt = target_tensor.T.to(device)
    #     tgt_input = tgt[:-1, :]

    #     src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)
    #     with torch.no_grad():
    #       logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)



    #     tgt_out = tgt[1:, :]
    #     loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))

    #     valid_losses +=loss.item()
    # valid_end_time = timer()




    scheduler.step(losses / len(list(train_dataloader)))
    #scheduler.step()

    return   transformer, optimizer, start_time, end_time, losses / len(list(train_dataloader))

torch.cuda.empty_cache()

model ={}
best_loss = - np.inf

batch_size = 16
input_lang, output_lang, train_dataloader = get_dataloader(batch_size = batch_size, label ="train")
# _, _, valid_dataloader = get_dataloader(batch_size = batch_size, label ="valid")

torch.cuda.empty_cache()

# NUM_EPOCHS = 100
# scheduler = CosineAnnealingWarmRestarts(optimizer, T_0 =NUM_EPOCHS , T_mult=1, verbose = True)
torch.cuda.empty_cache()
# model={1: transformer, 2: transformer, 3: transformer}
# experiments = 3


# for i in [1]:
    # data_1 = data.sort_values("score_{}".format(domain[i]), ascending = False)
    # data_1 = data_1[:1000]
    # data_finetune = data_1[["source_lang_{}".format(domain[i]), "target_lang"]]
    # data_finetune.rename(columns = {"source_lang_{}".format(domain[i]):"source_lang"}, inplace = True)
    # data_train_BT = pd.concat([data_train[i], data_finetune], ignore_index = True)


EMB_SIZE = 512
NHEAD = 8
FFN_HID_DIM = 1024

NUM_ENCODER_LAYERS = 2
NUM_DECODER_LAYERS = 2
torch.cuda.empty_cache()
SRC_VOCAB_SIZE = tokenizer.vocab_size
TGT_VOCAB_SIZE = tokenizer.vocab_size
# for z in [0,1,2]:
#     torch.cuda.empty_cache()
#     NUM_EPOCHS = 20
#     best_valid_loss = 1000
#     best_loss = 10
#     early_stop_threshold = 3
#     best_epoch =-1
#     j=0

#     torch.manual_seed(z)
#     SRC_VOCAB_SIZE = tokenizer.vocab_size
#     TGT_VOCAB_SIZE = tokenizer.vocab_size
#     # EMB_SIZE = 512
#     # NHEAD = 2
#     # FFN_HID_DIM = 512

#     # NUM_ENCODER_LAYERS = 2
#     # NUM_DECODER_LAYERS = 2

#     EMB_SIZE = 512
#     NHEAD = 8
#     FFN_HID_DIM = 1024

#     NUM_ENCODER_LAYERS = 2
#     NUM_DECODER_LAYERS = 2

#     DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


#     transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,
#                               NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)
#     for p in transformer.parameters():
#         if p.dim() > 1:
#             nn.init.xavier_uniform_(p)
#     transformer = transformer.to(DEVICE)
#     loss_fn = nn.CrossEntropyLoss()
#     optimizer = optim.Adam(transformer.parameters(), lr=0.0001)

#     scheduler = ReduceLROnPlateau(optimizer, patience=2, verbose = True)
#     # if z==1:
#         # North african BT_10K z=1 reminder 16 epochs
#     #     NUM_EPOCHS = 16
#     #     print("loading ...")
#     #     checkpoint = torch.load("./checkpoint_transformer_North_Africa_benchmark_BT_10000_{}.pt".format(z))
        
#     #     transformer.load_state_dict(checkpoint['model_state_dict'])
#     #     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
#     # if z==1:
#     #     print("loading ...")
#     #     checkpoint = torch.load("./checkpoint_transformer_North_Africa_benchmark_BT_10000_{}.pt".format(z))
        
#     #     transformer.load_state_dict(checkpoint['model_state_dict'])
#     #     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
#     #     NUM_EPOCHS = 21
#     #     # z=1 trained for 16 epochs instead of 37
#         #  
#     # Nile_basin z=1, trained on 29 epochs, base mode
#     print("loading ...")
#     checkpoint = torch.load("./checkpoint_transformer_Multidialect_benchmark_BT_5K_{}.pt".format(z))
    
#     transformer.load_state_dict(checkpoint['model_state_dict'])
#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
#     for epoch in range(1, NUM_EPOCHS+1):

#         if j < early_stop_threshold:
#             transformer, optimizer,  start_time, end_time, train_loss = train_epoch( transformer, optimizer )
    
#             # if (valid_loss < best_valid_loss ):
#             #   best_model = transformer
#             #   best_epoch = epoch
#             #   best_valid_loss = valid_loss
#             if train_loss < best_loss:
#                 print("Saving ...")
#                 best_loss = train_loss
                
#                 torch.save({
    
#                       'model_state_dict': transformer.state_dict(),
#                       'optimizer_state_dict': optimizer.state_dict(),
    
#                       }, "./checkpoint_transformer_Multidialect_benchmark_BT_5K_{}.pt".format(z))
#                 j=0
#             else:
#                 j = j+1
            
#         #   j = 0

#         # else:
#         #   j =j+1

#             print((f"Epoch: {epoch}, Train loss: {train_loss:.9f}, "f"Epoch time = {(end_time - start_time)/60:.3f}m") )
#         else:
#             break

def greedy_decode(model, src, src_mask,src_padding_mask, max_len, start_symbol, output_lang):
    src = src.to(DEVICE)
    src_mask = src_mask.to(DEVICE)
    decoded_words = []
    with torch.no_grad():
        memory = model.encode(src, src_mask,src_padding_mask)
    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)
    for i in range(max_len-1):
        memory = memory.to(DEVICE)
        tgt_mask = (generate_square_subsequent_mask(ys.size(0))
                    .type(torch.bool)).to(DEVICE)
        tgt_padding_mask = (ys == PAD_IDX).transpose(0, 1)
        out = model.decode(ys, memory, tgt_mask,tgt_padding_mask )
        out = out.transpose(0, 1)
        prob = model.generator(out[:, -1])
        _, next_word = torch.max(prob, dim=1)
        next_word = next_word.item()

        ys = torch.cat([ys,
                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)
        decoded_words.append(output_lang.index2word.get(next_word,output_lang.index2word[1]))
        if next_word == EOS_token:
            break
    return decoded_words, ys

def decode(model, src, src_mask,src_padding_mask, max_len, start_symbol, output_lang,
             p=None, greedy=None):
    """ Main decoding function, beam search is in a separate function """

    src = src.to(DEVICE)
    src_mask = src_mask.to(DEVICE)
    decoded_words = []
    with torch.no_grad():
        memory = model.encode(src, src_mask,src_padding_mask)
    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)
    for i in range(max_len-1):
        memory = memory.to(DEVICE)
        tgt_mask = (generate_square_subsequent_mask(ys.size(0))
                    .type(torch.bool)).to(DEVICE)
        tgt_padding_mask = (ys == PAD_IDX).transpose(0, 1)
        out = model.decode(ys, memory, tgt_mask,tgt_padding_mask )
        out = out.transpose(0, 1)
        logits = model.generator(out[:, -1])
        prob = F.softmax(logits, dim=-1)
        logprobs = F.log_softmax(logits, dim=-1)

        if greedy:
          _, next_word = torch.max(prob, dim=1)
          next_word = next_word.item()


        if p is not None:

          sorted_probs, sorted_indices = torch.sort(prob, descending=True)
          cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
          sorted_indices_to_remove = cumulative_probs > p
          sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()
          sorted_indices_to_remove[:, 0] = 0
          sorted_samp_probs = sorted_probs.clone()
          sorted_samp_probs[sorted_indices_to_remove] = 0



          sorted_next_indices = sorted_samp_probs.multinomial(1).view(-1, 1)

          next_tokens = sorted_indices.gather(1, sorted_next_indices)
          next_word = next_tokens.item()

          next_logprobs = sorted_samp_probs.gather(1, sorted_next_indices).log()

        ys = torch.cat([ys,
                          torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)
        decoded_words.append(output_lang.index2word.get(next_word,output_lang.index2word[1]))
        if next_word == EOS_token:
            break
    return decoded_words, ys

def beam_search(model, src, src_mask,src_padding_mask, max_len, start_symbol, output_lang, beam_width=5):
    with torch.no_grad():
        
        src = src.to(DEVICE)
        src_mask = src_mask.to(DEVICE)
        src_padding_mask = src_padding_mask.to(DEVICE)
        memory = model.encode(src, src_mask,src_padding_mask)
           
        ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)
        # Initialize the list to hold the completed beams
        completed_beams = []
        
        # Start the beam search process
        
        candidate_beams = []
            # decoder_hidden = encoder_hidden
            
            # Get the top-k candidates and their probabilities
            # topk_probs, topk_indices = decoder_outputs.squeeze().topk(beam_width)
            # topk_probs = topk_probs.T #(beam, max_length)
            # topk_indices = topk_indices.T
    
            # Expand the current beam with the top-k candidates
    
            # for i in range(1, beam_width+1):
        
        for i in range(beam_width):
            memory = model.encode(src, src_mask,src_padding_mask)
               
            ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)
            
            decoder_output=[]
            candidate_prob = 0.0
            for j in range(MAX_LENGTH):
                memory = memory.to(DEVICE)
                tgt_mask = (generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(DEVICE)
                tgt_padding_mask = (ys == PAD_IDX).transpose(0, 1)
                out = model.decode(ys, memory, tgt_mask,tgt_padding_mask )
                out = out.transpose(0, 1)
                logits = model.generator(out[:, -1])
                prob = F.softmax(logits, dim=-1)
                logprobs = F.log_softmax(logits, dim=-1) #batch_size, vocab
                topk_probs, topk_indices = logprobs.squeeze().topk(beam_width) # beam_width
                new_candidate_idx = topk_indices[i].item()
                
                ys = torch.cat([ys,torch.ones(1, 1).type_as(src.data).fill_(new_candidate_idx)], dim=0)
                
                decoder_output.append(topk_indices[i].item())
                candidate_prob += topk_probs[i].item()
                new_beam_score= candidate_prob
                if new_candidate_idx == EOS_token:
                    completed_beams.append((decoder_output, new_beam_score))
                    break
                if j == (MAX_LENGTH-1):
                    completed_beams.append((decoder_output, new_beam_score))
                # decoded_words.append(output_lang.index2word.get(next_word,output_lang.index2word[1]))
            
            # new_candidates_idxs = list(topk_indices.cpu().numpy())
            # decoder_output.append(new_candidates_idxs)
            
            # candidate_prob += topk_probs[i][j].item()
            # new_decoder_input = torch.tensor([decoder_output], device=device)
            # new_beam_score =(candidate_prob)
            
            # Check if the candidate is the end token
            # if new_candidate_idx == EOS_token:
            #     completed_beams.append((decoder_output, new_beam_score))
            #     break
            # if j == (MAX_LENGTH-1):
            #     completed_beams.append((decoder_output, new_beam_score))
    
    # Sort the completed beams and select the one with the highest score
    completed_beams.sort(key=lambda x: x[1], reverse=True)
    
    # decoded_words = [output_lang.index2word.get(word,tokenizer.unk_token)for word in completed_beams[0][0]]
    decoded_words = [output_lang.index2word.get(word,output_lang.index2word[1]) for  word in completed_beams[0][0]]
    return decoded_words  # No attention scores for beam search

       
# actual function to translate input sentence into target language
def translate(model: torch.nn.Module, data, input_lang, output_lang, beam=None, n=None):
    targets =[]
    outputs =[]
    model.eval()
    i=0
    for idx in data.index:
      src_sentence = data.source_lang[idx]
      trgt = data.target_lang[idx]
      if i == n:
          break

      src = tensorFromSentence(input_lang, normalizeString(src_sentence)).view(-1, 1)
      num_tokens = src.shape[0]
      src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)
      src_padding_mask = (src == PAD_IDX).transpose(0, 1)
      
      if beam:
        decoded_words = beam_search(model, src, src_mask,src_padding_mask, num_tokens + 5,
                                    SOS_token, output_lang, beam_width=beam)
      
      else:
          
        decoded_words, _ = decode(
            model,  src, src_mask,src_padding_mask,max_len=num_tokens + 5,
            start_symbol=SOS_token, output_lang=output_lang, greedy = True
            #p=0.95
            )
      output_words = tokenizer.convert_tokens_to_string(decoded_words)
      output_words = output_words.replace("[SEP]", "")
      output_words = output_words.replace("[UNK]", "")
      output_words = output_words.replace("[PAD]", "")
      # print("input: ", src_sentence)
      # print("target: ", trgt)
      # print("prediction: ", output_words)
      # print('')
      i=i+1
      targets.append([normalizeString(trgt)])
      outputs.append(output_words)
  
    return outputs, targets

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive

from nltk.translate.bleu_score import corpus_bleu
from torchmetrics.text import SacreBLEUScore, BLEUScore
import sacrebleu
# Predicted sentences
preds = ['the cat is on the mat']

# Reference sentences (note: each reference list contains a list of reference sentences)
references = [['there is a cat on the mat']]

# Initialize SacreBLEUScore
sacre_bleu = SacreBLEUScore()

# Calculate the BLEU score
score = sacre_bleu(preds, references)
sacrebleu.corpus_bleu(preds, references)
print(f"SacreBLEU score: {score}")
import sacrebleu

preds = ['the cat is on the mat']
tgt = [['there is a cat on the mat']]
sacre_bleu = SacreBLEUScore()
sacre_bleu(preds, tgt)

# for i in [1]:
#     # bleu={
#     #     "Gulf": [],
#     #         "Iraqi": [],
#     #         "Levantine":[],
#     #         "Nile_Basin":[],
#     #         "North_Africa":[]
#     #         }
#     bleu={
#             "Gulf":[],

#           }

#     # model_path_list = ["./checkpoint_transformer_{}_{}.pt".format(domain[i],x) for x in [0,1,2]]
#     # model_path_list = ["./checkpoint_transformer_Multidialect_benchmark_BT_5K_{}.pt".format(x)for x in [0,1,2]]

#     model_path_list = ["./checkpoint_transformer_Gulf_benchmark_BT10K_{}.pt".format(x)for x in [0,1,2]]
   

#     # checkpoint = torch.load("./checkpoint_transformer_finetune_arabert_large_MADAR_2_{}_BT_4_2_{}.pt".format(domain[i],z))
#     z=0
#     # model_path_list= ["checkpoint_transformer_Gulf.pt"]
#     for path in model_path_list:

#         torch.manual_seed(z)
#         z +=1
#         transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,
#                                  NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)
#         for p in transformer.parameters():
#             if p.dim() > 1:
#                 nn.init.xavier_uniform_(p)
#         transformer = transformer.to(DEVICE)
#         loss_fn = nn.CrossEntropyLoss()
#         optimizer = optim.Adam(transformer.parameters(), lr=0.0001)
#         checkpoint = torch.load(path)
#         print("loading")
#         transformer.load_state_dict(checkpoint['model_state_dict'])


#         for j, (k, v) in enumerate(bleu.items()):

#             data = data_test[0]
#             outputs, target= translate(transformer, data, input_lang, output_lang, beam=6)
#             src_test = corpus_bleu(target, outputs)
#             # src_test_sacrebleu = sacrebleu.corpus_bleu(outputs, target).score
#             bleu[k].extend([src_test] )
#             print(bleu)
#             src_test

bleu={
      "Gulf": [],
          "Iraqi": [],
          "Levantine":[],
          "Nile_Basin":[],
          "North_Africa":[]}

# model_path_list = ["./checkpoint_transformer_{}_{}.pt".format(domain[i],x) for x in [0,1,2]]
# model_path_list = ["./checkpoint_transformer_Multidialect_benchmark_BT_5K_{}.pt".format(x)for x in [0,1,2]]

# for j, (k,v) in enumerate(bleu.items()):
    # if k == "Gulf":
    #     # model_path_list = ["./checkpoint_transformer_Gulf_benchmark_BT10K_{}.pt".format(x)for x in [0,1,2]]
    #     continue
    # elif k == "Levantine":
    #     model_path_list = ["./checkpoint_transformer_lev_benchmark_{}.pt".format(x)for x in [0,1,2]]
    #     model_path_list = model_path_list + ["./checkpoint_transformer_lev_benchmark_BT_10K_{}.pt".format(x)for x in [0,1,2]]
    # elif k == "North_Africa":
    #     model_path_list = ["./checkpoint_transformer_{}_benchmark_{}.pt".format(k,x)for x in [0,1,2]]
    #     model_path_list = model_path_list + ["./checkpoint_transformer_{}_benchmark_BT_10000_{}.pt".format(k,x)for x in [0,1,2]] 
    #     model_path_list = model_path_list + ["./checkpoint_transformer_{}_benchmark_BT_5K_{}.pt".format(k,x)for x in [0,1,2]]    
    # elif k == "Iraqi":
        
    #     model_path_list =["./checkpoint_transformer_{}_benchmark_BT_10K_{}.pt".format(k,x)for x in [0,1,2]]
    # elif k == "Nile_Basin":
    #     model_path_list = ["./checkpoint_transformer_{}_benchmark_{}.pt".format(k,x)for x in [0,1,2]]
    #     model_path_list = model_path_list + ["./checkpoint_transformer_{}_benchmark_BT_10K_{}.pt".format(k,x)for x in [0,1,2]]
# checkpoint = torch.load("./checkpoint_transformer_finetune_arabert_large_MADAR_2_{}_BT_4_2_{}.pt".format(domain[i],z))
    


model_path_list = ["./checkpoint_transformer_Multidialect_benchmark_{}.pt".format(x) for x in [0,1,2]]
model_path_list = model_path_list + ["./checkpoint_transformer_Multidialect_benchmark_BT_5K_{}.pt".format(x) for x in [0,1,2]]
 
z=0
# model_path_list= ["checkpoint_transformer_Gulf.pt"]
# output = {k:[]}
for path in model_path_list:

    torch.manual_seed(z)
    z +=1
    transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,
                             NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)
    for p in transformer.parameters():
        if p.dim() > 1:
            nn.init.xavier_uniform_(p)
    transformer = transformer.to(DEVICE)
    loss_fn = nn.CrossEntropyLoss()
    optimizer = optim.Adam(transformer.parameters(), lr=0.0001)
    checkpoint = torch.load(path)
    print("loading")
    transformer.load_state_dict(checkpoint['model_state_dict'])
    for j, (k,v) in enumerate(bleu.items()):
        data = data_test[j]
        outputs, target= translate(transformer, data, input_lang, output_lang, beam=6)
        src_test = corpus_bleu(target, outputs)
        # src_test_sacrebleu = sacrebleu.corpus_bleu(outputs, target).score
        bleu[k].extend([src_test])
        print(bleu)
        src_test
        
#Multi-dialect benchmark
# {'Gulf': [0.746396317547805, 0.7491863105641015, 0.7535241963848623, 0.7443221752654633], 'Iraqi': [0.8415177771744583, 0.8461270710498078, 0.8491731720500112, 0.8505804813645912], 'Levantine': [0.7671753850442438, 0.7737541326172459, 0.7792010526574144], 'Nile_Basin': [0.7424644934316111, 0.7434083806982679, 0.7492252645441837], 'North_Africa': [0.6838305695649466, 0.682675654712296, 0.6916864097326234]}
#Multi-dialect benchmark BT
# {'Gulf': [0.7443221752654633, 0.7507089706023363, 0.7501566079555209], 'Iraqi': [0.8505804813645912, 0.841144284729301, 0.8425340849448215], 'Levantine': [ 0.765032223005525, 0.7760206139927384, 0.7773296545764801], 'Nile_Basin': [0.7487585160839771, 0.7553288788048198, 0.7500450641137637], 'North_Africa': [0.6835921219622406, 0.6813420370477271, 0.6847082938971509]}

# combined
# {'Gulf': [0.746396317547805, 0.7491863105641015, 0.7535241963848623, 0.7443221752654633, 0.7507089706023363, 0.7501566079555209], 'Iraqi': [0.8415177771744583, 0.8461270710498078, 0.8491731720500112, 0.8505804813645912, 0.841144284729301, 0.8425340849448215], 'Levantine': [0.7671753850442438, 0.7737541326172459, 0.7792010526574144, 0.765032223005525, 0.7760206139927384, 0.7773296545764801], 'Nile_Basin': [0.7424644934316111, 0.7434083806982679, 0.7492252645441837, 0.7487585160839771, 0.7553288788048198, 0.7500450641137637], 'North_Africa': [0.6838305695649466, 0.682675654712296, 0.6916864097326234, 0.6835921219622406, 0.6813420370477271, 0.6847082938971509]}
    

# beam =6, checkpoint_transformer_Gulf_benchmark_0,1,.pt
# {'Gulf': [0.5683762488076101, 0.5666197561122025, 0.5676131715510363 ] }           
# beam =6, checkpoint_transformer_Gulf_benchmark_BT10K_,1,.pt
# {'Gulf': [0.5766851924792562, 0.5807558527502249, 0.5807940961329567]}   
# beam =6, checkpoint_transformer_Iraqi_benchmark_0
# {'Iraqi': [0.6461598779816322, 0.6318035497001462, 0.6599829551384465]}
# beam =6, checkpoint_transformer_Iraqi__benchmark_BT_10K_0
# {'Iraqi': [0.6172478369633633, 0.6094536390277204, 0.6186993613907978]}
# Levantine, benchmark
# {'Levantine': [0.6431390373556517, 0.6407111554271507, 0.6452098299406226]}
# Levantine, benchmark_BT
# {'Levantine': [0.6521689560460032, 0.6482642059748731, 0.6427109107755997]}
# Nile_Basin, benchmark
# {'Nile_Basin': [0.570351188273614, 0.5691184763209, 0.5698367629326708]}
# Nile_Basin, benchmark_BT
# {'Nile_Basin': [0.5757263446015799, 0.582452864435006, 0.5754679076299457]}
# North_Africa, benchmark
# {'North_Africa': [0.6014352806813118, 0.6010183679297189, 0.6010070491265948]}
# North_Africa, _benchmark_BT_10000_
# {'North_Africa': [0.5978404888010296, 0.6051585313875282, 0.6005516284556296]}
# North_Africa, _benchmark_BT_5K_
# {'North_Africa': [ 0.6116923875425831, 0.6050199779641992, 0.6087028555187018]}





# beam =6, bleu, sacrebleu, Gulf, z=0, checkpoint_transformer_Gulf_benchmark_0
# {'Gulf': [0.5683762488076101, 69.14415692838826]}

# from sacrebleu.metrics import BLEU, CHRF, TER
# bleu = BLEU()

# bleu.corpus_score(outputs, target)

# target
# outputs
# bleu
# bleu_= BLEUScore()
# bleu_(outputs, target)
# target[0]
# sacre_bleu(outputs, target)
# sacrebleu.corpus_bleu(outputs, target).score
# outputs[:2]
# target[:2]
# sacre_bleu(outputs, target)
# sacre_bleu( [target[0]], [outputs[0]])
# # Multi-dialect base
# {'Gulf': [0.7454915340220346, 0.7484479856852505, 0.752821291337745], 'Iraqi': [0.8403708529098031, 0.8450045694973818, 0.848044559127659], 'Levantine': [0.7663163172979393, 0.7731679110430449, 0.7786590156770958], 'Nile_Basin': [0.7423131643015531, 0.7431376452834734, 0.7491448177355909], 'North_Africa': [0.68362102747533, 0.6823378875858578, 0.6914195696011332]}
# Multi-dialect BT_5K
# {'Gulf': [0.7435124614422179, 0.7498573133070697, 0.749464101532834], 'Iraqi': [0.8494156355993534, 0.8400366163731572, 0.8414152921703794], 'Levantine': [0.7644508913634364, 0.7753869170461702, 0.7767542250589737], 'Nile_Basin': [0.7487024414017628, 0.7550146599585472, 0.7498982982186732], 'North_Africa': [0.6834236289218385, 0.6812749721143244, 0.6845863727869408]}

# {'Iraqi': [0.6460886550028752, 0.6317452420574287, 0.6599325523246001]}, base
# {'Iraqi': [0.6173687177288929, 0.6094316892540792, 0.6187502840288015]}, BT_10K

# {'Nile_Basin': [0.5702091862737896, 0.5691030936366953, 0.5696451548845496]}, base
# {'Nile_Basin': [0.575753538970825, 0.5824500116531878, 0.5755210942470711]} BT_10K

# {'North_Africa': [0.6013722378800087, 0.6009956311567582, 0.6009137615884463]} base
# {'North_Africa': [0.597834871711751, 0.595037238856517, 0.6005517943674353]}  BT_10K
# {'North_Africa': [0.6116769726857165, 0.6049539768813144, 0.6085958680293689]} BT 5K

# {'Gulf': [0.5678764244378696, 0.5662067039162866, 0.5669879697479104]} base
# {'Gulf': [0.5763564872244354, 0.5801661882805326, 0.5801960340943657]} BT_10K


# {'Levantine': [0.6427566276076133, 0.6404043717506891, 0.6446455245239553]} base
# {'Levantine': [0.6516784704591557, 0.6479184302089313, 0.6422254528614754]} BT_10K, benchmark domain cosine
# {'Levantine': [0.6438501126652671, 0.6363741845612728, 0.6386639535043351]}, BT_5K

# {'Levantine': [0.6449112443114645]} BT 5000
# {'Levantine': [0.6549761757545951, 0.6464040129291757, 0.641593738930135]} BT10000
# {'Levantine': [0.6442581538197049]} BT20000


# {'Levantine': [0.6266225140324939]}  greedy True

# 0.6426770997508492 Levantine score Base model
# 0.6408133824189108 Levantine score BT model

data_test