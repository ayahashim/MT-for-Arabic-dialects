{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRtz0P4Nh7Lc",
        "outputId": "b2282d6f-b357-4741-c56a-2c07bb96ebc7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from transformers import *\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "import time\n",
        "import os\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFKC', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
        "    s = re.sub(r'[\" \"]+', \" \", s)\n",
        "\n",
        "    s = re.sub(r\"[^a-zA-Z؀-ۿ?.!,¿]+\", \" \", s)\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    # s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "# encode the training data for each domain\n",
        "def convert_data_to_features(data, tokenizer,\n",
        "                                      max_length=200,\n",
        "                                      pad_token=0,\n",
        "                                      pad_token_segment_id=0,\n",
        "                                      mask_padding_with_zero=True):\n",
        "    # if max_lines_to_encode:\n",
        "    #     examples = open(file_path, 'r').readlines()[:max_lines_to_encode]\n",
        "    # else:\n",
        "    #     examples = open(file_path, 'r').readlines()\n",
        "    features = []\n",
        "\n",
        "    for idx in (data.index):\n",
        "        sent = normalizeString(data[\"target_lang\"][idx])\n",
        "        if idx % 10000 == 0:\n",
        "            print(\"Writing example %d\" % (idx))\n",
        "        inputs = tokenizer.encode_plus(\n",
        "            text=sent,\n",
        "            add_special_tokens=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length)\n",
        "        input_ids = inputs[\"input_ids\"]\n",
        "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "        padding_length = max_length - len(input_ids)\n",
        "        input_ids = input_ids + ([pad_token] * padding_length)\n",
        "        attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
        "        # token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
        "        assert len(input_ids) == max_length, \"Error with input length {} vs {}\".format(len(input_ids), max_length)\n",
        "        assert len(attention_mask) == max_length, \"Error with input length {} vs {}\".format(len(attention_mask), max_length)\n",
        "        # assert len(token_type_ids) == max_length, \"Error with input length {} vs {}\".format(len(token_type_ids), max_length)\n",
        "        features.append(\n",
        "                InputFeatures(input_ids=input_ids,\n",
        "                              attention_mask=attention_mask,\n",
        "\n",
        "                              # token_type_ids=token_type_ids,\n",
        "                              ))\n",
        "\n",
        "\n",
        "    return features\n",
        "\n",
        "def features_to_tensor_dataset(features):\n",
        "    # Convert to Tensors and build dataset\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
        " \n",
        "    dataset = TensorDataset(all_input_ids, all_attention_mask)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def run_batched_inference(tensor_dataset, model,batch_size=100):\n",
        "   \n",
        "    # setup device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if device == 'cpu':\n",
        "        print('using cpu!!!')\n",
        "    else:\n",
        "        print('using gpu.')\n",
        "    model.to(device)\n",
        "\n",
        "    # Start inference loop and sample data sequentially starting from index 0\n",
        "    inference_sampler = SequentialSampler(tensor_dataset)\n",
        "    inference_dataloader = DataLoader(tensor_dataset, sampler=inference_sampler, batch_size=batch_size)\n",
        "    avg_pooled_all = []\n",
        "\n",
        "    for batch in tqdm(inference_dataloader, desc=\"Running inference...\"):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            inputs = {'input_ids': batch[0],\n",
        "                      'attention_mask': batch[1]}\n",
        "                        'labels': batch[3]}\n",
        "            \n",
        "            outputs = model(**inputs)[0] # of shape [batch_size, seq_len, state_size]\n",
        "\n",
        "            # Compute avg pooling\n",
        "            avg_pooled_batch = []\n",
        "            for i in range(outputs.shape[0]):\n",
        "                seq_len = inputs['attention_mask'][i].sum().item()\n",
        "                avg_pooled_batch.append(outputs[i][range(seq_len),:].mean(dim=0).cpu().numpy())\n",
        "            avg_pooled_batch = np.stack(avg_pooled_batch)\n",
        "            avg_pooled_all.append(avg_pooled_batch)\n",
        "\n",
        "\n",
        "    avg_pooled_all = np.concatenate(avg_pooled_all)\n",
        "\n",
        "    print(avg_pooled_all.shape)\n",
        "    return avg_pooled_all\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vp0Q3x-dmlQJ"
      },
      "outputs": [],
      "source": [
        "def encode_data_and_save(data,tokenizer, model, batch_size =512):\n",
        " \n",
        "    input_features = convert_data_to_features(data, tokenizer,\n",
        "                                                   max_length=200,\n",
        "                                                   )\n",
        "    tensor_dataset = features_to_tensor_dataset(input_features)\n",
        "    start = time.time()\n",
        "    avg_pooled = run_batched_inference(tensor_dataset,model, batch_size=batch_size)\n",
        "    end = time.time()\n",
        "\n",
        "    return avg_pooled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U98f6QAzFyQJ"
      },
      "outputs": [],
      "source": [
        "def run_batched_inference_sent(tensor_dataset, model,batch_size=100):\n",
        "    # Load pretrained model\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if device == 'cpu':\n",
        "        print('using cpu!!!')\n",
        "    else:\n",
        "        print('using gpu.')\n",
        "    model.to(device)\n",
        "\n",
        "    # Start inference loop and sample data sequentially starting from index 0\n",
        "    inference_sampler = SequentialSampler(tensor_dataset)\n",
        "    inference_dataloader = DataLoader(tensor_dataset, sampler=inference_sampler, batch_size=batch_size)\n",
        "    avg_pooled_all = []\n",
        "\n",
        "    for batch in tqdm(inference_dataloader, desc=\"Running inference...\"):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            inputs = {'input_ids': batch[0],\n",
        "                      'attention_mask': batch[1]}\n",
        "#                       \n",
        "            outputs = model(**inputs)[0] # of shape [batch_size, state_size]\n",
        "\n",
        "            # Compute avg pooling\n",
        "            avg_pooled_batch = []\n",
        "            for i in range(outputs.shape[0]):\n",
        "                seq_len = inputs['attention_mask'][i].sum().item()\n",
        "                avg_pooled_batch.append(outputs[i].cpu().numpy())\n",
        "            avg_pooled_batch = np.stack(avg_pooled_batch)\n",
        "            avg_pooled_all.append(avg_pooled_batch)\n",
        "\n",
        "\n",
        "    avg_pooled_all = np.concatenate(avg_pooled_all)\n",
        "\n",
        "    print(avg_pooled_all.shape)\n",
        "    return avg_pooled_all\n",
        "\n",
        "def encode_data_and_save_sent(data,tokenizer, model, batch_size =512):\n",
        "    # tokenizer = AutoTokenizer.from_pretrained('aubmindlab/bert-large-arabertv02')\n",
        "    input_features = convert_data_to_features(data, tokenizer,\n",
        "                                                   max_length=200,\n",
        "                                                   )\n",
        "    tensor_dataset = features_to_tensor_dataset(input_features)\n",
        "    start = time.time()\n",
        "    avg_pooled = run_batched_inference(tensor_dataset,model, batch_size=batch_size)\n",
        "    end = time.time()\n",
        "    # os.system('mkdir -p {}'.format('/'.join(output_path.split('/')[:-1])))\n",
        "    # print('encoded in {} seconds'.format(end - start))\n",
        "    # np.save(output_path, avg_pooled)\n",
        "    return avg_pooled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Fp7BDeHjZP_"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "from peft import get_peft_model\n",
        "from peft import LoraConfig, TaskType\n",
        "\n",
        "# Load Arabert_model\n",
        "model_arabert = AutoModel.from_pretrained('aubmindlab/bert-large-arabertv02')\n",
        "tokenizer_arabert = AutoTokenizer.from_pretrained('aubmindlab/bert-large-arabertv02')\n",
        "lora_config = LoraConfig(task_type=TaskType.FEATURE_EXTRACTION, r=64, lora_alpha=1, lora_dropout=0.01)\n",
        "model_arabert_lora = get_peft_model(model_arabert, lora_config)\n",
        "\n",
        "checkpoint = torch.load(\"./checkpoint_sent_arabert_large.pt\")\n",
        "model_arabert_lora.load_state_dict(checkpoint[\"model_dict\"])\n",
        "# optimizer.load_state_dict(checkpoint[\"optimizer_dict\"])\n",
        "# loss = checkpoint[\"loss\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_ISZe1NGdsh",
        "outputId": "b929d1af-bf17-4565-9847-d7dd418e7b2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing example 0\n",
            "Writing example 10000\n",
            "Writing example 20000\n",
            "Writing example 30000\n",
            "Writing example 40000\n",
            "Writing example 50000\n",
            "using gpu.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running inference...: 100%|██████████| 112/112 [36:40<00:00, 19.65s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(57143, 1024)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "data_all = pd.read_csv(\"./data_selected_translated_filtered_all\", sep =\"\\t\")\n",
        "\n",
        "avg_pooled_domain = encode_data_and_save(data_all, tokenizer_arabert, model_arabert_lora)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9zTyv_OHaG-"
      },
      "outputs": [],
      "source": [
        "save_format = './corpora_sent_arabert_large_all.npy'\n",
        "np.save(save_format, avg_pooled_domain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVUucEozII4Y",
        "outputId": "349377b5-b844-4ed8-8757-0d4242730c24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing example 0\n",
            "using gpu.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running inference...: 100%|██████████| 6/6 [01:52<00:00, 18.77s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2922, 1024)\n",
            "Writing example 0\n",
            "Writing example 10000\n",
            "using gpu.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running inference...: 100%|██████████| 34/34 [10:54<00:00, 19.25s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(17002, 1024)\n",
            "Writing example 0\n",
            "using gpu.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running inference...: 100%|██████████| 19/19 [06:10<00:00, 19.51s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(9659, 1024)\n",
            "Writing example 0\n",
            "using gpu.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running inference...: 100%|██████████| 20/20 [06:22<00:00, 19.11s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(9946, 1024)\n",
            "Writing example 0\n",
            "Writing example 10000\n",
            "Writing example 20000\n",
            "using gpu.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running inference...:  45%|████▍     | 26/58 [08:32<10:30, 19.70s/it]"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "domain = [\"Iraqi\", \"Levantine\", \"Gulf\", \"Nile_Basin\", \"North_Africa\"]\n",
        "data_domain = []\n",
        "\n",
        "for i in range(len(domain)):\n",
        "  data_domain.append(pd.read_csv(\"./data_{}_preprocessed_train\".format(domain[i]), sep=\"\\t\"))\n",
        "\n",
        "\n",
        "for i in range(len(data_domain)):\n",
        "  avg_pooled_domain = encode_data_and_save(data_domain[i], tokenizer_arabert, model_arabert_lora)\n",
        "  save_format = './{}_sent_arabert_large_in_domain.npy'.format(domain[i])\n",
        "  np.save(save_format, avg_pooled_domain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ds1d8dDIOgD"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "save_format_corpra = './corpora_sent_arabert_large_all.npy'\n",
        "corpra_pooled = np.load(save_format_corpra)\n",
        "\n",
        "for i in range(len(domain)):\n",
        "  save_format_domain = './{}_sent_arabert_large_in_domain.npy'.format(domain[i])\n",
        "  in_domain_pooled = np.load(save_format_domain)\n",
        "  dev_centroid = np.mean(in_domain_pooled, axis = 0)\n",
        "  data_all[\"score_{}\".format(domain[i])] = [0 for _ in range(data_all.shape[0])]\n",
        "  for j in range(corpra_pooled.shape[0]):\n",
        "    vecs = corpra_pooled[j].reshape(1,-1)\n",
        "    corpra_sent = data_all.target_lang[j]\n",
        "    score = cosine_similarity(dev_centroid.reshape(1, -1), vecs)[0][0]\n",
        "    data_all[\"score_{}\".format(domain[i])][j] = score\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
